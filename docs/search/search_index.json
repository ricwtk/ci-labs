{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CSC3034 Computational Intelligence This site hosts the lab sheets for the module of CSC3034 Computational Intelligence in the Department of Computing and Information Systems (DCIS) in Sunway University. Aim The aim of these labs is to guide the students to implement the basic computational intelligence (CI) algorithms with and/or without Python libraries. Information The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab. Schedule The schedule is subject to change. Week 2 Lab 1 Week 4 Lab 2 Week 6 Lab 3 Week 7 Lab 4 Week 8 Lab 5 Week 9 Lab 6 Week 10 Lab 7 Week 12 Lab 8","title":"Overview"},{"location":"#aim","text":"The aim of these labs is to guide the students to implement the basic computational intelligence (CI) algorithms with and/or without Python libraries.","title":"Aim"},{"location":"#information","text":"The labs are designed to follow the schedule of the lectures, therefore you will require the knowledge of the previous lectures to be able to conduct the lab.","title":"Information"},{"location":"#schedule","text":"The schedule is subject to change. Week 2 Lab 1 Week 4 Lab 2 Week 6 Lab 3 Week 7 Lab 4 Week 8 Lab 5 Week 9 Lab 6 Week 10 Lab 7 Week 12 Lab 8","title":"Schedule"},{"location":"lab1/","text":"Lab 1: Refresh on Python Amoeba community Assuming a new amoeba takes one month to grow, and from the second month onwards, it takes one month to duplicate itself to create a new amoeba. Given that there is one new amoeba at the beginning of the first month, this is the progression of the number of amoeba in different months. Month 0 : 1 (new) Month 1 : 1 (grown) Month 2 : 1 (grown) + 1 (new) = 2 Month 3 : 2 (grown) + 1 (new) = 3 Month 4 : 3 (grown) + 2 (new) = 5 Month 5 : 5 (grown) + 3 (new) = 8 Month 6 : 8 (grown) + 5 (new) = 13 Month 7 : 13 (grown) + 8 (new) = 21 ... Hint Note the pattern of the sequence Write a function that takes the month number as input argument and provides the number of amoeba at the beginning of that month as output. 1 2 3 def numberofamoeba ( month ): ... return number_of_amoeba Write a function to take the same input argument as numberofamoeba but instead of giving the number of amoeba at that month as output, provide the whole sequence of amoeba number starting from the beginning. For example, if month is 4 , the output of the function should be the list of [1,1,2,3,5] 1 2 3 def numberofamoebaseq ( month ): ... return number_seq Create a scatter plot to plot the sequence of amoeba number from month 0 to month 100. Hint import matplotlib.pyplot as plt to use the Python visualisation library Matplotlib. Scatter plot can be produced with plt.scatter(...) . Fibonacci and Golden Ratio The above sequence of number is also known as the Fibonacci sequence. Note A Fibonacci sequence may or may not include a 0 as the first element of the series, i.e. 0,1,1,2,3,5,8,... instead of 1,1,2,3,5,8 . Plot the ratio between every two consecutive numbers in the Fibonacci sequence. For Fibonacci sequence of 1,1,2,3,5,8,13,21 , plot the line of \\(\\frac{1}{1}\\) , \\(\\frac{2}{1}\\) , \\(\\frac{3}{2}\\) , \\(\\frac{5}{3}\\) , \\(\\frac{8}{5}\\) , \\(\\frac{13}{8}\\) , \\(\\frac{21}{13}\\) . Note The longer the Fibonacci sequence you use, the closer is the value of the ratio between two consecutive numbers to be the golden ratio. Generate a series of coordinates following the algorithm: Start from (0,0) . Get the next Fibonacci number, i.e. 1 . Add (+1,+1) to the previous point (0,0) to get (1,1) . Get the next Fibonacci number, i.e. 1 . Add (+1,-1) to the previous point (1,1) to get (2,0) . Get the next Fibonacci number, i.e. 2 . Add (-2,-2) to the previous point (2,0) to get (0,-2) . Get the next Fibonacci number, i.e. 3 . Add (-3,+3) to the previous point (0,-2) to get (-3,1) . Continue with the next Fibonacci number and update the coordinates with the sequence of the signs (+,+), (+,-), (-,-), (-,+) . The process will create a spiral in the following manner. The sequence of the signs produce the change in directions, and the fibonacci number provides the distance. (0,0) (1,1) (2,0) (0,-2) (-3,1) Create a line plot of the series of coordinates. If the lines are smoothen, it would form the golden spiral which can be found in pinecorns, seashells, and hurricanes. Additional If you are interested in how we may plot arc to connect the points instead of using straight lines, you can refer to Additional: plot arc to form golden spiral . Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption Additional: plot arc to form golden spiral The golden spiral can be produced by drawing the arc connecting every consecutive coordinates. (0,0) (1,1) (2,0) (0,-2) (-3,1) (1,0) (1,0) To draw the arc using matplotlib library, we need to identify the center of each arc. The arc and its corresponding center are colored with the same color in the previous figure. matplotlib . patches . Arc ( xy , # center of the arc width , # length of horizontal axis, height , # length of vertical axis, angle , # rotation of the ellipse in degrees (counterclockwise) theta1 , # starting angle of the arc in degrees theta2 # end angle of the arc in degrees ) The centers of every arc can be genrated from the sequence of coordinates using the following function: function generatecenters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generatecenters ( coordinates ): centers = [] for i , coord in enumerate ( coordinates ): if i == 0 : # add coordinate to list of center centers . append ([ coord [ 0 ], coord [ 1 ]]) elif i == 1 : # change x-coordinate of the first center centers [ - 1 ][ 0 ] = coord [ 0 ] else : centers . append ([ centers [ - 1 ][ 0 ], centers [ - 1 ][ 1 ]]) if i % 2 == 0 : # use y-coordinate as y for new center centers [ - 1 ][ 1 ] = coord [ 1 ] else : # use x-coordinate as x for new center centers [ - 1 ][ 0 ] = coord [ 0 ] return centers The coordinates is the list of coordinates generated from Fibonacci and Golden Ratio step 3. The following function will then use the generated centers of the arc, and the Fibonacci sequence generated from numberofamoebaseq to draw the arc. The handler of the axis needs to be passed into the function as well. function plotspiral 1 2 3 4 5 6 7 8 9 10 11 12 13 def plotspiral ( axis , series , centers ): angle = 90 for number , center in zip ( series , centers ): arc = Arc ( xy = center , width = 2 * number , height = 2 * number , angle = angle , theta1 = 0 , theta2 = 90 ) axis . add_patch ( arc ) angle -= 90 In your script, you will first generate the Fibonacci sequence, use the sequence to generate coordinates, generate centers of arcs, and plot the arcs to form the spiral. 1 2 3 4 5 6 7 n = 80 number_seq = numberofamoebaseq ( n ) coordinates = generatecoordinatesfromseries ( number_seq ) centers = generatecenters ( coordinates ) plt . figure () plt . scatter ( ... ) # or plt.plot(...) to plot the coordinates as in Fibonacci and Golden Ratio step 4 plotspiral ( plt . gca (), number_seq , centers ) # plt.gca() returns handle of the current axis Limitation Due to the limitation of matplotlib, the spiral plotting only works for the Fibonacci sequence with length less than 93.","title":"Lab 1: Refresh on Python"},{"location":"lab1/#lab-1-refresh-on-python","text":"","title":"Lab 1: Refresh on Python"},{"location":"lab1/#amoeba-community","text":"Assuming a new amoeba takes one month to grow, and from the second month onwards, it takes one month to duplicate itself to create a new amoeba. Given that there is one new amoeba at the beginning of the first month, this is the progression of the number of amoeba in different months. Month 0 : 1 (new) Month 1 : 1 (grown) Month 2 : 1 (grown) + 1 (new) = 2 Month 3 : 2 (grown) + 1 (new) = 3 Month 4 : 3 (grown) + 2 (new) = 5 Month 5 : 5 (grown) + 3 (new) = 8 Month 6 : 8 (grown) + 5 (new) = 13 Month 7 : 13 (grown) + 8 (new) = 21 ... Hint Note the pattern of the sequence Write a function that takes the month number as input argument and provides the number of amoeba at the beginning of that month as output. 1 2 3 def numberofamoeba ( month ): ... return number_of_amoeba Write a function to take the same input argument as numberofamoeba but instead of giving the number of amoeba at that month as output, provide the whole sequence of amoeba number starting from the beginning. For example, if month is 4 , the output of the function should be the list of [1,1,2,3,5] 1 2 3 def numberofamoebaseq ( month ): ... return number_seq Create a scatter plot to plot the sequence of amoeba number from month 0 to month 100. Hint import matplotlib.pyplot as plt to use the Python visualisation library Matplotlib. Scatter plot can be produced with plt.scatter(...) .","title":"Amoeba community"},{"location":"lab1/#fibonacci-and-golden-ratio","text":"The above sequence of number is also known as the Fibonacci sequence. Note A Fibonacci sequence may or may not include a 0 as the first element of the series, i.e. 0,1,1,2,3,5,8,... instead of 1,1,2,3,5,8 . Plot the ratio between every two consecutive numbers in the Fibonacci sequence. For Fibonacci sequence of 1,1,2,3,5,8,13,21 , plot the line of \\(\\frac{1}{1}\\) , \\(\\frac{2}{1}\\) , \\(\\frac{3}{2}\\) , \\(\\frac{5}{3}\\) , \\(\\frac{8}{5}\\) , \\(\\frac{13}{8}\\) , \\(\\frac{21}{13}\\) . Note The longer the Fibonacci sequence you use, the closer is the value of the ratio between two consecutive numbers to be the golden ratio. Generate a series of coordinates following the algorithm: Start from (0,0) . Get the next Fibonacci number, i.e. 1 . Add (+1,+1) to the previous point (0,0) to get (1,1) . Get the next Fibonacci number, i.e. 1 . Add (+1,-1) to the previous point (1,1) to get (2,0) . Get the next Fibonacci number, i.e. 2 . Add (-2,-2) to the previous point (2,0) to get (0,-2) . Get the next Fibonacci number, i.e. 3 . Add (-3,+3) to the previous point (0,-2) to get (-3,1) . Continue with the next Fibonacci number and update the coordinates with the sequence of the signs (+,+), (+,-), (-,-), (-,+) . The process will create a spiral in the following manner. The sequence of the signs produce the change in directions, and the fibonacci number provides the distance. (0,0) (1,1) (2,0) (0,-2) (-3,1) Create a line plot of the series of coordinates. If the lines are smoothen, it would form the golden spiral which can be found in pinecorns, seashells, and hurricanes. Additional If you are interested in how we may plot arc to connect the points instead of using straight lines, you can refer to Additional: plot arc to form golden spiral .","title":"Fibonacci and Golden Ratio"},{"location":"lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"lab1/#additional-plot-arc-to-form-golden-spiral","text":"The golden spiral can be produced by drawing the arc connecting every consecutive coordinates. (0,0) (1,1) (2,0) (0,-2) (-3,1) (1,0) (1,0) To draw the arc using matplotlib library, we need to identify the center of each arc. The arc and its corresponding center are colored with the same color in the previous figure. matplotlib . patches . Arc ( xy , # center of the arc width , # length of horizontal axis, height , # length of vertical axis, angle , # rotation of the ellipse in degrees (counterclockwise) theta1 , # starting angle of the arc in degrees theta2 # end angle of the arc in degrees ) The centers of every arc can be genrated from the sequence of coordinates using the following function: function generatecenters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generatecenters ( coordinates ): centers = [] for i , coord in enumerate ( coordinates ): if i == 0 : # add coordinate to list of center centers . append ([ coord [ 0 ], coord [ 1 ]]) elif i == 1 : # change x-coordinate of the first center centers [ - 1 ][ 0 ] = coord [ 0 ] else : centers . append ([ centers [ - 1 ][ 0 ], centers [ - 1 ][ 1 ]]) if i % 2 == 0 : # use y-coordinate as y for new center centers [ - 1 ][ 1 ] = coord [ 1 ] else : # use x-coordinate as x for new center centers [ - 1 ][ 0 ] = coord [ 0 ] return centers The coordinates is the list of coordinates generated from Fibonacci and Golden Ratio step 3. The following function will then use the generated centers of the arc, and the Fibonacci sequence generated from numberofamoebaseq to draw the arc. The handler of the axis needs to be passed into the function as well. function plotspiral 1 2 3 4 5 6 7 8 9 10 11 12 13 def plotspiral ( axis , series , centers ): angle = 90 for number , center in zip ( series , centers ): arc = Arc ( xy = center , width = 2 * number , height = 2 * number , angle = angle , theta1 = 0 , theta2 = 90 ) axis . add_patch ( arc ) angle -= 90 In your script, you will first generate the Fibonacci sequence, use the sequence to generate coordinates, generate centers of arcs, and plot the arcs to form the spiral. 1 2 3 4 5 6 7 n = 80 number_seq = numberofamoebaseq ( n ) coordinates = generatecoordinatesfromseries ( number_seq ) centers = generatecenters ( coordinates ) plt . figure () plt . scatter ( ... ) # or plt.plot(...) to plot the coordinates as in Fibonacci and Golden Ratio step 4 plotspiral ( plt . gca (), number_seq , centers ) # plt.gca() returns handle of the current axis Limitation Due to the limitation of matplotlib, the spiral plotting only works for the Fibonacci sequence with length less than 93.","title":"Additional: plot arc to form golden spiral"},{"location":"archive/201908/lab1/","text":"Lab 1: Pre-lab Refresh on Python Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python funciton to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Lab 1: Pre-lab"},{"location":"archive/201908/lab1/#lab-1-pre-lab","text":"","title":"Lab 1: Pre-lab"},{"location":"archive/201908/lab1/#refresh-on-python","text":"Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq","title":"Refresh on Python"},{"location":"archive/201908/lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python funciton to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"archive/201908/lab2/","text":"Lab 2: EC (GA) Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( offsprings )","title":"Lab 2: EC (GA)"},{"location":"archive/201908/lab2/#lab-2-ec-ga","text":"","title":"Lab 2: EC (GA)"},{"location":"archive/201908/lab2/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"archive/201908/lab2/#genetic-algorithm","text":"Consider the following problem: You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"archive/201908/lab2/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14","title":"Feature encoding"},{"location":"archive/201908/lab2/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"archive/201908/lab2/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"archive/201908/lab2/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"archive/201908/lab2/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"archive/201908/lab2/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"archive/201908/lab2/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"archive/201908/lab2/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( offsprings )","title":"Combining all functions"},{"location":"archive/201908/lab3/","text":"Lab 3: EC (PSO) Particle swarm optimisation Objective develop a Python function to perform global best particle swarm optimisation Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Find the value of x to minimise the function f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100) for -100 < x < 100 Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by x i (t+1) = x i (t) + v i (t+1) and the velocity update function is v i (t+1) = v i (t) + \u03b1 1 \u03b2 1 (t) ( p i (t) - x i (t) ) + \u03b1 2 \u03b2 2 (t) ( p g (t) - x i (t) ) \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def calc_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Boxplot for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, v i (t), with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \u03b1 1 to 0.05 while maintaining \u03b1 2 at 0.1 and investigate the effect. Reduce the value of \u03b1 1 to 0. How does this affect the result? Modify such that \u03b1 1 is larger than \u03b1 2 . What's the effect? Optional How may you modify the formulae for particles with two variables, in which the fitness function is defined as f(x,y) = x 2 + y 2 ?","title":"Lab 3: EC (PSO)"},{"location":"archive/201908/lab3/#lab-3-ec-pso","text":"","title":"Lab 3: EC (PSO)"},{"location":"archive/201908/lab3/#particle-swarm-optimisation","text":"","title":"Particle swarm optimisation"},{"location":"archive/201908/lab3/#objective","text":"develop a Python function to perform global best particle swarm optimisation","title":"Objective"},{"location":"archive/201908/lab3/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/201908/lab3/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Find the value of x to minimise the function f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100) for -100 < x < 100","title":"Problem to solve"},{"location":"archive/201908/lab3/#particle-swarm-optimisation_1","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/201908/lab3/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by x i (t+1) = x i (t) + v i (t+1) and the velocity update function is v i (t+1) = v i (t) + \u03b1 1 \u03b2 1 (t) ( p i (t) - x i (t) ) + \u03b1 2 \u03b2 2 (t) ( p g (t) - x i (t) ) \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/201908/lab3/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/201908/lab3/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/201908/lab3/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/201908/lab3/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/201908/lab3/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/201908/lab3/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/201908/lab3/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def calc_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/201908/lab3/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/201908/lab3/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/201908/lab3/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Boxplot for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/201908/lab3/#exercise","text":"Multiply the velocity memory, v i (t), with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \u03b1 1 to 0.05 while maintaining \u03b1 2 at 0.1 and investigate the effect. Reduce the value of \u03b1 1 to 0. How does this affect the result? Modify such that \u03b1 1 is larger than \u03b1 2 . What's the effect?","title":"Exercise"},{"location":"archive/201908/lab3/#optional","text":"How may you modify the formulae for particles with two variables, in which the fitness function is defined as f(x,y) = x 2 + y 2 ?","title":"Optional"},{"location":"archive/201908/lab4/","text":"Lab 4: EC (ACO) Ant colony optimisation Objective to develop a Python function to perform ant colony optimisation on a problem Problem to solve We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" }); Problem formulation The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ] Initiating ACO algorithm We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )] Identify path of each ant In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = [] Evaporation In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road Deposition In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant k on road i: \u0394\u03c4 i,k = 1/L k 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone Termination conditions We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage Loop until termination Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ... Visualisation Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 ) Evaluate effect of parameters Modify the pheromone depositing formula to \u0394\u03c4 i,k = 1/L k 1.5 What is the effect of this? Modify the pheromone depositing formula to \u0394\u03c4 i,k = 5/L k What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \u03b1 alpha . Investigate the effect of evaporation rate \u03c1 rho .","title":"Lab 4: EC (ACO)"},{"location":"archive/201908/lab4/#lab-4-ec-aco","text":"","title":"Lab 4: EC (ACO)"},{"location":"archive/201908/lab4/#ant-colony-optimisation","text":"","title":"Ant colony optimisation"},{"location":"archive/201908/lab4/#objective","text":"to develop a Python function to perform ant colony optimisation on a problem","title":"Objective"},{"location":"archive/201908/lab4/#problem-to-solve","text":"We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" });","title":"Problem to solve"},{"location":"archive/201908/lab4/#problem-formulation","text":"The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ]","title":"Problem formulation"},{"location":"archive/201908/lab4/#initiating-aco-algorithm","text":"We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )]","title":"Initiating ACO algorithm"},{"location":"archive/201908/lab4/#identify-path-of-each-ant","text":"In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = []","title":"Identify path of each ant"},{"location":"archive/201908/lab4/#evaporation","text":"In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road","title":"Evaporation"},{"location":"archive/201908/lab4/#deposition","text":"In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant k on road i: \u0394\u03c4 i,k = 1/L k 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone","title":"Deposition"},{"location":"archive/201908/lab4/#termination-conditions","text":"We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage","title":"Termination conditions"},{"location":"archive/201908/lab4/#loop-until-termination","text":"Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ...","title":"Loop until termination"},{"location":"archive/201908/lab4/#visualisation","text":"Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 )","title":"Visualisation"},{"location":"archive/201908/lab4/#evaluate-effect-of-parameters","text":"Modify the pheromone depositing formula to \u0394\u03c4 i,k = 1/L k 1.5 What is the effect of this? Modify the pheromone depositing formula to \u0394\u03c4 i,k = 5/L k What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \u03b1 alpha . Investigate the effect of evaporation rate \u03c1 rho .","title":"Evaluate effect of parameters"},{"location":"archive/201908/lab5/","text":"Lab 5: ANN (Supervised learning) Artificial neural networks Objective to construct an multi-layer perceptron classifier using the scikit-learn Python library Load data to be learned We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine () Examine the dataset We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ()) Split data into training and testing sets scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth. Data preprocessing From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset? Construct the ANN model As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train ) Predictions The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test ) Evaluation With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report? Parameters of the fitted model The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs Visualisation the neural network Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp ) Compare the weights before and after the training At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations. Effect of parameters Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/201908/lab5/#lab-5-ann-supervised-learning","text":"","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/201908/lab5/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/201908/lab5/#objective","text":"to construct an multi-layer perceptron classifier using the scikit-learn Python library","title":"Objective"},{"location":"archive/201908/lab5/#load-data-to-be-learned","text":"We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine ()","title":"Load data to be learned"},{"location":"archive/201908/lab5/#examine-the-dataset","text":"We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ())","title":"Examine the dataset"},{"location":"archive/201908/lab5/#split-data-into-training-and-testing-sets","text":"scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth.","title":"Split data into training and testing sets"},{"location":"archive/201908/lab5/#data-preprocessing","text":"From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset?","title":"Data preprocessing"},{"location":"archive/201908/lab5/#construct-the-ann-model","text":"As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 )","title":"Construct the ANN model"},{"location":"archive/201908/lab5/#train-the-model","text":"The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train )","title":"Train the model"},{"location":"archive/201908/lab5/#predictions","text":"The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test )","title":"Predictions"},{"location":"archive/201908/lab5/#evaluation","text":"With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report?","title":"Evaluation"},{"location":"archive/201908/lab5/#parameters-of-the-fitted-model","text":"The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs","title":"Parameters of the fitted model"},{"location":"archive/201908/lab5/#visualisation-the-neural-network","text":"Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp )","title":"Visualisation the neural network"},{"location":"archive/201908/lab5/#compare-the-weights-before-and-after-the-training","text":"At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations.","title":"Compare the weights before and after the training"},{"location":"archive/201908/lab5/#effect-of-parameters","text":"Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Effect of parameters"},{"location":"archive/201908/lab6/","text":"Lab 6: ANN (Hyperplane) Artificial neural networks Objective to visualise the hyperplanes of a neural network configuration for better understanding Data preparation We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target Setup the first configuration for neural network Data preprocessing Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the ANN model Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train ) Visualise the classification of a fitted model Prepare the figure and axis 1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) Visualisation function Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis Visualise the result Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Setup the second, third, and more neural network configurations We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test . Consider more input features We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the model 1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train ) Visualise decision area with more input features We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features? Additional There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/201908/lab6/#lab-6-ann-hyperplane","text":"","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/201908/lab6/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/201908/lab6/#objective","text":"to visualise the hyperplanes of a neural network configuration for better understanding","title":"Objective"},{"location":"archive/201908/lab6/#data-preparation","text":"We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target","title":"Data preparation"},{"location":"archive/201908/lab6/#setup-the-first-configuration-for-neural-network","text":"","title":"Setup the first configuration for neural network"},{"location":"archive/201908/lab6/#data-preprocessing","text":"Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Data preprocessing"},{"location":"archive/201908/lab6/#construct-and-train-the-ann-model","text":"Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train )","title":"Construct and train the ANN model"},{"location":"archive/201908/lab6/#visualise-the-classification-of-a-fitted-model","text":"","title":"Visualise the classification of a fitted model"},{"location":"archive/201908/lab6/#prepare-the-figure-and-axis","text":"1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 )","title":"Prepare the figure and axis"},{"location":"archive/201908/lab6/#visualisation-function","text":"Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis","title":"Visualisation function"},{"location":"archive/201908/lab6/#visualise-the-result","text":"Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test )","title":"Visualise the result"},{"location":"archive/201908/lab6/#setup-the-second-third-and-more-neural-network-configurations","text":"We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test .","title":"Setup the second, third, and more neural network configurations"},{"location":"archive/201908/lab6/#consider-more-input-features","text":"We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Consider more input features"},{"location":"archive/201908/lab6/#construct-and-train-the-model","text":"1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train )","title":"Construct and train the model"},{"location":"archive/201908/lab6/#visualise-decision-area-with-more-input-features","text":"We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features?","title":"Visualise decision area with more input features"},{"location":"archive/201908/lab6/#additional","text":"There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Additional"},{"location":"archive/201908/lab7/","text":"Lab 7: Fuzzy Systems Fuzzy Systems Objective to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system Note As of 3 rd Nov 2019, the scikit-fuzzy Python library only work properly with Python 3.7 and networkx Python library of version 2.3. networkx version 2.4 is not supported yet. To create a separate environment in Anaconda for scikit-fuzzy library, run the following code in the Anaconda prompt. 1 2 3 conda create - n fuzzy python = 3.7 networkx = 2.3 spyder conda activate fuzzy conda install - c conda - forge scikit - fuzzy Launch Spyder IDE using the command spyder . Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Lab 7: Fuzzy Systems"},{"location":"archive/201908/lab7/#lab-7-fuzzy-systems","text":"","title":"Lab 7: Fuzzy Systems"},{"location":"archive/201908/lab7/#fuzzy-systems","text":"","title":"Fuzzy Systems"},{"location":"archive/201908/lab7/#objective","text":"to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system","title":"Objective"},{"location":"archive/201908/lab7/#note","text":"As of 3 rd Nov 2019, the scikit-fuzzy Python library only work properly with Python 3.7 and networkx Python library of version 2.3. networkx version 2.4 is not supported yet. To create a separate environment in Anaconda for scikit-fuzzy library, run the following code in the Anaconda prompt. 1 2 3 conda create - n fuzzy python = 3.7 networkx = 2.3 spyder conda activate fuzzy conda install - c conda - forge scikit - fuzzy Launch Spyder IDE using the command spyder .","title":"Note"},{"location":"archive/201908/lab7/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/201908/lab7/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/201908/lab7/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/201908/lab7/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/201908/lab7/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/201908/lab7/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/201908/lab7/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Fuzzy tipping recommendation system"},{"location":"archive/202008/lab1/","text":"Lab 1: Pre-lab Refresh on Python Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python funciton to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption Submission Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Lab 1: Pre-lab"},{"location":"archive/202008/lab1/#lab-1-pre-lab","text":"","title":"Lab 1: Pre-lab"},{"location":"archive/202008/lab1/#refresh-on-python","text":"Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq","title":"Refresh on Python"},{"location":"archive/202008/lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python funciton to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"archive/202008/lab1/#submission","text":"Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Submission"},{"location":"archive/202008/lab2/","text":"Lab 2: EC (GA) Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Lab 2: EC (GA)"},{"location":"archive/202008/lab2/#lab-2-ec-ga","text":"","title":"Lab 2: EC (GA)"},{"location":"archive/202008/lab2/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"archive/202008/lab2/#genetic-algorithm","text":"Consider the following problem: You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"archive/202008/lab2/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14","title":"Feature encoding"},{"location":"archive/202008/lab2/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"archive/202008/lab2/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"archive/202008/lab2/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"archive/202008/lab2/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"archive/202008/lab2/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"archive/202008/lab2/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"archive/202008/lab2/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Combining all functions"},{"location":"archive/202008/lab3/","text":"Lab 3: EC (PSO) Particle swarm optimisation Objective develop a Python function to perform global best particle swarm optimisation Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Find the value of x to minimise the function f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100) for -100 < x < 100 Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by x i (t+1) = x i (t) + v i (t+1) and the velocity update function is v i (t+1) = v i (t) + \u03b1 1 \u03b2 1 (t) ( p i (t) - x i (t) ) + \u03b1 2 \u03b2 2 (t) ( p g (t) - x i (t) ) \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def calc_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Boxplot for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, v i (t), with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \u03b1 1 to 0.05 while maintaining \u03b1 2 at 0.1 and investigate the effect. Reduce the value of \u03b1 1 to 0. How does this affect the result? Modify such that \u03b1 1 is larger than \u03b1 2 . What's the effect? Optional How may you modify the formulae for particles with two variables, in which the fitness function is defined as f(x,y) = x 2 + y 2 ?","title":"Lab 3: EC (PSO)"},{"location":"archive/202008/lab3/#lab-3-ec-pso","text":"","title":"Lab 3: EC (PSO)"},{"location":"archive/202008/lab3/#particle-swarm-optimisation","text":"","title":"Particle swarm optimisation"},{"location":"archive/202008/lab3/#objective","text":"develop a Python function to perform global best particle swarm optimisation","title":"Objective"},{"location":"archive/202008/lab3/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/202008/lab3/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Find the value of x to minimise the function f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100) for -100 < x < 100","title":"Problem to solve"},{"location":"archive/202008/lab3/#particle-swarm-optimisation_1","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/202008/lab3/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by x i (t+1) = x i (t) + v i (t+1) and the velocity update function is v i (t+1) = v i (t) + \u03b1 1 \u03b2 1 (t) ( p i (t) - x i (t) ) + \u03b1 2 \u03b2 2 (t) ( p g (t) - x i (t) ) \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/202008/lab3/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/202008/lab3/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/202008/lab3/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/202008/lab3/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/202008/lab3/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/202008/lab3/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/202008/lab3/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def calc_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/202008/lab3/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/202008/lab3/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/202008/lab3/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Boxplot for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/202008/lab3/#exercise","text":"Multiply the velocity memory, v i (t), with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \u03b1 1 to 0.05 while maintaining \u03b1 2 at 0.1 and investigate the effect. Reduce the value of \u03b1 1 to 0. How does this affect the result? Modify such that \u03b1 1 is larger than \u03b1 2 . What's the effect?","title":"Exercise"},{"location":"archive/202008/lab3/#optional","text":"How may you modify the formulae for particles with two variables, in which the fitness function is defined as f(x,y) = x 2 + y 2 ?","title":"Optional"},{"location":"archive/202008/lab4/","text":"Lab 4: EC (ACO) Ant colony optimisation Objective to develop a Python function to perform ant colony optimisation on a problem Problem to solve We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" }); Problem formulation The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ] Initiating ACO algorithm We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )] Identify path of each ant In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = [] Evaporation In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road Deposition In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant k on road i: \u0394\u03c4 i,k = 1/L k 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone Termination conditions We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage Loop until termination Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ... Visualisation Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 ) Evaluate effect of parameters Modify the pheromone depositing formula to \u0394\u03c4 i,k = 1/L k 1.5 What is the effect of this? Modify the pheromone depositing formula to \u0394\u03c4 i,k = 5/L k What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \u03b1 alpha . Investigate the effect of evaporation rate \u03c1 rho .","title":"Lab 4: EC (ACO)"},{"location":"archive/202008/lab4/#lab-4-ec-aco","text":"","title":"Lab 4: EC (ACO)"},{"location":"archive/202008/lab4/#ant-colony-optimisation","text":"","title":"Ant colony optimisation"},{"location":"archive/202008/lab4/#objective","text":"to develop a Python function to perform ant colony optimisation on a problem","title":"Objective"},{"location":"archive/202008/lab4/#problem-to-solve","text":"We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" });","title":"Problem to solve"},{"location":"archive/202008/lab4/#problem-formulation","text":"The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ]","title":"Problem formulation"},{"location":"archive/202008/lab4/#initiating-aco-algorithm","text":"We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )]","title":"Initiating ACO algorithm"},{"location":"archive/202008/lab4/#identify-path-of-each-ant","text":"In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = []","title":"Identify path of each ant"},{"location":"archive/202008/lab4/#evaporation","text":"In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road","title":"Evaporation"},{"location":"archive/202008/lab4/#deposition","text":"In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant k on road i: \u0394\u03c4 i,k = 1/L k 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone","title":"Deposition"},{"location":"archive/202008/lab4/#termination-conditions","text":"We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage","title":"Termination conditions"},{"location":"archive/202008/lab4/#loop-until-termination","text":"Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ...","title":"Loop until termination"},{"location":"archive/202008/lab4/#visualisation","text":"Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 )","title":"Visualisation"},{"location":"archive/202008/lab4/#evaluate-effect-of-parameters","text":"Modify the pheromone depositing formula to \u0394\u03c4 i,k = 1/L k 1.5 What is the effect of this? Modify the pheromone depositing formula to \u0394\u03c4 i,k = 5/L k What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \u03b1 alpha . Investigate the effect of evaporation rate \u03c1 rho .","title":"Evaluate effect of parameters"},{"location":"archive/202008/lab5/","text":"Lab 5: ANN (Supervised learning) Artificial neural networks Objective to construct an multi-layer perceptron classifier using the scikit-learn Python library Load data to be learned We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine () Examine the dataset We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ()) Split data into training and testing sets scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth. Data preprocessing From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset? Construct the ANN model As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train ) Predictions The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test ) Evaluation With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report? Parameters of the fitted model The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs Visualisation the neural network Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp ) Compare the weights before and after the training At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations. Effect of parameters Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202008/lab5/#lab-5-ann-supervised-learning","text":"","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202008/lab5/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202008/lab5/#objective","text":"to construct an multi-layer perceptron classifier using the scikit-learn Python library","title":"Objective"},{"location":"archive/202008/lab5/#load-data-to-be-learned","text":"We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine ()","title":"Load data to be learned"},{"location":"archive/202008/lab5/#examine-the-dataset","text":"We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ())","title":"Examine the dataset"},{"location":"archive/202008/lab5/#split-data-into-training-and-testing-sets","text":"scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth.","title":"Split data into training and testing sets"},{"location":"archive/202008/lab5/#data-preprocessing","text":"From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset?","title":"Data preprocessing"},{"location":"archive/202008/lab5/#construct-the-ann-model","text":"As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 )","title":"Construct the ANN model"},{"location":"archive/202008/lab5/#train-the-model","text":"The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train )","title":"Train the model"},{"location":"archive/202008/lab5/#predictions","text":"The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test )","title":"Predictions"},{"location":"archive/202008/lab5/#evaluation","text":"With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report?","title":"Evaluation"},{"location":"archive/202008/lab5/#parameters-of-the-fitted-model","text":"The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs","title":"Parameters of the fitted model"},{"location":"archive/202008/lab5/#visualisation-the-neural-network","text":"Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp )","title":"Visualisation the neural network"},{"location":"archive/202008/lab5/#compare-the-weights-before-and-after-the-training","text":"At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations.","title":"Compare the weights before and after the training"},{"location":"archive/202008/lab5/#effect-of-parameters","text":"Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Effect of parameters"},{"location":"archive/202008/lab6/","text":"Lab 6: ANN (Hyperplane) Artificial neural networks Objective to visualise the hyperplanes of a neural network configuration for better understanding Data preparation We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target Setup the first configuration for neural network Data preprocessing Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the ANN model Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train ) Visualise the classification of a fitted model Prepare the figure and axis 1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) Visualisation function Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis Visualise the result Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Setup the second, third, and more neural network configurations We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test . Consider more input features We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the model 1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train ) Visualise decision area with more input features We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features? Additional There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202008/lab6/#lab-6-ann-hyperplane","text":"","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202008/lab6/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202008/lab6/#objective","text":"to visualise the hyperplanes of a neural network configuration for better understanding","title":"Objective"},{"location":"archive/202008/lab6/#data-preparation","text":"We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target","title":"Data preparation"},{"location":"archive/202008/lab6/#setup-the-first-configuration-for-neural-network","text":"","title":"Setup the first configuration for neural network"},{"location":"archive/202008/lab6/#data-preprocessing","text":"Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Data preprocessing"},{"location":"archive/202008/lab6/#construct-and-train-the-ann-model","text":"Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train )","title":"Construct and train the ANN model"},{"location":"archive/202008/lab6/#visualise-the-classification-of-a-fitted-model","text":"","title":"Visualise the classification of a fitted model"},{"location":"archive/202008/lab6/#prepare-the-figure-and-axis","text":"1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 )","title":"Prepare the figure and axis"},{"location":"archive/202008/lab6/#visualisation-function","text":"Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis","title":"Visualisation function"},{"location":"archive/202008/lab6/#visualise-the-result","text":"Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test )","title":"Visualise the result"},{"location":"archive/202008/lab6/#setup-the-second-third-and-more-neural-network-configurations","text":"We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test .","title":"Setup the second, third, and more neural network configurations"},{"location":"archive/202008/lab6/#consider-more-input-features","text":"We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Consider more input features"},{"location":"archive/202008/lab6/#construct-and-train-the-model","text":"1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train )","title":"Construct and train the model"},{"location":"archive/202008/lab6/#visualise-decision-area-with-more-input-features","text":"We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features?","title":"Visualise decision area with more input features"},{"location":"archive/202008/lab6/#additional","text":"There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Additional"},{"location":"archive/202008/lab7/","text":"Lab 7: Fuzzy Systems Fuzzy Systems Objective to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202008/lab7/#lab-7-fuzzy-systems","text":"","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202008/lab7/#fuzzy-systems","text":"","title":"Fuzzy Systems"},{"location":"archive/202008/lab7/#objective","text":"to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system","title":"Objective"},{"location":"archive/202008/lab7/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"archive/202008/lab7/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/202008/lab7/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/202008/lab7/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/202008/lab7/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/202008/lab7/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/202008/lab7/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/202008/lab7/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Fuzzy tipping recommendation system"},{"location":"archive/202108/lab1/","text":"Lab 1: Pre-lab Refresh on Python Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption Submission Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Lab 1: Pre-lab"},{"location":"archive/202108/lab1/#lab-1-pre-lab","text":"","title":"Lab 1: Pre-lab"},{"location":"archive/202108/lab1/#refresh-on-python","text":"Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq","title":"Refresh on Python"},{"location":"archive/202108/lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"archive/202108/lab1/#submission","text":"Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Submission"},{"location":"archive/202108/lab2/","text":"Lab 2: EC (GA) Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Lab 2: EC (GA)"},{"location":"archive/202108/lab2/#lab-2-ec-ga","text":"","title":"Lab 2: EC (GA)"},{"location":"archive/202108/lab2/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"archive/202108/lab2/#genetic-algorithm","text":"Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"archive/202108/lab2/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14","title":"Feature encoding"},{"location":"archive/202108/lab2/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"archive/202108/lab2/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"archive/202108/lab2/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"archive/202108/lab2/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"archive/202108/lab2/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"archive/202108/lab2/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"archive/202108/lab2/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Combining all functions"},{"location":"archive/202108/lab3/","text":"Lab 3: EC (PSO) Particle swarm optimisation Objective develop a Python function to perform global best particle swarm optimisation Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\) Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? Optional How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Lab 3: EC (PSO)"},{"location":"archive/202108/lab3/#lab-3-ec-pso","text":"","title":"Lab 3: EC (PSO)"},{"location":"archive/202108/lab3/#particle-swarm-optimisation","text":"","title":"Particle swarm optimisation"},{"location":"archive/202108/lab3/#objective","text":"develop a Python function to perform global best particle swarm optimisation","title":"Objective"},{"location":"archive/202108/lab3/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/202108/lab3/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\)","title":"Problem to solve"},{"location":"archive/202108/lab3/#particle-swarm-optimisation_1","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/202108/lab3/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/202108/lab3/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/202108/lab3/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/202108/lab3/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/202108/lab3/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/202108/lab3/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/202108/lab3/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/202108/lab3/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/202108/lab3/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/202108/lab3/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/202108/lab3/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/202108/lab3/#exercise","text":"Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect?","title":"Exercise"},{"location":"archive/202108/lab3/#optional","text":"How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Optional"},{"location":"archive/202108/lab4/","text":"Lab 4: EC (ACO) Ant colony optimisation Objective to develop a Python function to perform ant colony optimisation on a problem Problem to solve We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" }); Problem formulation The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ] Initiating ACO algorithm We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )] Identify path of each ant In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = [] Evaporation In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road Deposition In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone Termination conditions We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage Loop until termination Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ... Visualisation Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 ) Evaluate effect of parameters Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Lab 4: EC (ACO)"},{"location":"archive/202108/lab4/#lab-4-ec-aco","text":"","title":"Lab 4: EC (ACO)"},{"location":"archive/202108/lab4/#ant-colony-optimisation","text":"","title":"Ant colony optimisation"},{"location":"archive/202108/lab4/#objective","text":"to develop a Python function to perform ant colony optimisation on a problem","title":"Objective"},{"location":"archive/202108/lab4/#problem-to-solve","text":"We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" });","title":"Problem to solve"},{"location":"archive/202108/lab4/#problem-formulation","text":"The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ]","title":"Problem formulation"},{"location":"archive/202108/lab4/#initiating-aco-algorithm","text":"We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )]","title":"Initiating ACO algorithm"},{"location":"archive/202108/lab4/#identify-path-of-each-ant","text":"In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = []","title":"Identify path of each ant"},{"location":"archive/202108/lab4/#evaporation","text":"In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road","title":"Evaporation"},{"location":"archive/202108/lab4/#deposition","text":"In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone","title":"Deposition"},{"location":"archive/202108/lab4/#termination-conditions","text":"We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage","title":"Termination conditions"},{"location":"archive/202108/lab4/#loop-until-termination","text":"Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ...","title":"Loop until termination"},{"location":"archive/202108/lab4/#visualisation","text":"Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 )","title":"Visualisation"},{"location":"archive/202108/lab4/#evaluate-effect-of-parameters","text":"Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Evaluate effect of parameters"},{"location":"archive/202108/lab5/","text":"Lab 5: ANN (Supervised learning) Artificial neural networks Objective to construct an multi-layer perceptron classifier using the scikit-learn Python library Load data to be learned We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine () Examine the dataset We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ()) Split data into training and testing sets scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth. Data preprocessing From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset? Construct the ANN model As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train ) Predictions The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test ) Evaluation With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report? Parameters of the fitted model The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs Visualisation the neural network Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp ) Compare the weights before and after the training At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations. Effect of parameters Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202108/lab5/#lab-5-ann-supervised-learning","text":"","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202108/lab5/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202108/lab5/#objective","text":"to construct an multi-layer perceptron classifier using the scikit-learn Python library","title":"Objective"},{"location":"archive/202108/lab5/#load-data-to-be-learned","text":"We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine ()","title":"Load data to be learned"},{"location":"archive/202108/lab5/#examine-the-dataset","text":"We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ())","title":"Examine the dataset"},{"location":"archive/202108/lab5/#split-data-into-training-and-testing-sets","text":"scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth.","title":"Split data into training and testing sets"},{"location":"archive/202108/lab5/#data-preprocessing","text":"From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset?","title":"Data preprocessing"},{"location":"archive/202108/lab5/#construct-the-ann-model","text":"As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 )","title":"Construct the ANN model"},{"location":"archive/202108/lab5/#train-the-model","text":"The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train )","title":"Train the model"},{"location":"archive/202108/lab5/#predictions","text":"The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test )","title":"Predictions"},{"location":"archive/202108/lab5/#evaluation","text":"With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report?","title":"Evaluation"},{"location":"archive/202108/lab5/#parameters-of-the-fitted-model","text":"The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs","title":"Parameters of the fitted model"},{"location":"archive/202108/lab5/#visualisation-the-neural-network","text":"Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp )","title":"Visualisation the neural network"},{"location":"archive/202108/lab5/#compare-the-weights-before-and-after-the-training","text":"At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations.","title":"Compare the weights before and after the training"},{"location":"archive/202108/lab5/#effect-of-parameters","text":"Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Effect of parameters"},{"location":"archive/202108/lab6/","text":"Lab 6: ANN (Hyperplane) Artificial neural networks Objective to visualise the hyperplanes of a neural network configuration for better understanding Data preparation We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target Setup the first configuration for neural network Data preprocessing Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the ANN model Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train ) Visualise the classification of a fitted model Prepare the figure and axis 1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) Visualisation function Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis Visualise the result Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Setup the second, third, and more neural network configurations We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test . Consider more input features We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the model 1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train ) Visualise decision area with more input features We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features? Additional There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202108/lab6/#lab-6-ann-hyperplane","text":"","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202108/lab6/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202108/lab6/#objective","text":"to visualise the hyperplanes of a neural network configuration for better understanding","title":"Objective"},{"location":"archive/202108/lab6/#data-preparation","text":"We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target","title":"Data preparation"},{"location":"archive/202108/lab6/#setup-the-first-configuration-for-neural-network","text":"","title":"Setup the first configuration for neural network"},{"location":"archive/202108/lab6/#data-preprocessing","text":"Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Data preprocessing"},{"location":"archive/202108/lab6/#construct-and-train-the-ann-model","text":"Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train )","title":"Construct and train the ANN model"},{"location":"archive/202108/lab6/#visualise-the-classification-of-a-fitted-model","text":"","title":"Visualise the classification of a fitted model"},{"location":"archive/202108/lab6/#prepare-the-figure-and-axis","text":"1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 )","title":"Prepare the figure and axis"},{"location":"archive/202108/lab6/#visualisation-function","text":"Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis","title":"Visualisation function"},{"location":"archive/202108/lab6/#visualise-the-result","text":"Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test )","title":"Visualise the result"},{"location":"archive/202108/lab6/#setup-the-second-third-and-more-neural-network-configurations","text":"We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test .","title":"Setup the second, third, and more neural network configurations"},{"location":"archive/202108/lab6/#consider-more-input-features","text":"We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Consider more input features"},{"location":"archive/202108/lab6/#construct-and-train-the-model","text":"1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train )","title":"Construct and train the model"},{"location":"archive/202108/lab6/#visualise-decision-area-with-more-input-features","text":"We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features?","title":"Visualise decision area with more input features"},{"location":"archive/202108/lab6/#additional","text":"There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Additional"},{"location":"archive/202108/lab7/","text":"Lab 7: Fuzzy Systems Fuzzy Systems Objective to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202108/lab7/#lab-7-fuzzy-systems","text":"","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202108/lab7/#fuzzy-systems","text":"","title":"Fuzzy Systems"},{"location":"archive/202108/lab7/#objective","text":"to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system","title":"Objective"},{"location":"archive/202108/lab7/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"archive/202108/lab7/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/202108/lab7/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/202108/lab7/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/202108/lab7/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/202108/lab7/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/202108/lab7/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/202108/lab7/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Fuzzy tipping recommendation system"},{"location":"archive/202208/lab1/","text":"Lab 1: Pre-lab Refresh on Python Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption Submission Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Lab 1: Pre-lab"},{"location":"archive/202208/lab1/#lab-1-pre-lab","text":"","title":"Lab 1: Pre-lab"},{"location":"archive/202208/lab1/#refresh-on-python","text":"Fibonacci sequence is a series of numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... The first two numbers are 0 and 1. The rest of the numbers in the sequence is found by summing up two numbers before it. Create a function, in Python, which takes the sequence length sl as the input of the function and prints the Fibonacci sequence of the length sl . 1 2 3 def fibonacci ( sl ): ... return fib_seq","title":"Refresh on Python"},{"location":"archive/202208/lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"archive/202208/lab1/#submission","text":"Submit a Python file with the three functions: fibonacci , tossCoin , and chooseFromThree .","title":"Submission"},{"location":"archive/202208/lab2/","text":"Lab 2: EC (GA) Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Lab 2: EC (GA)"},{"location":"archive/202208/lab2/#lab-2-ec-ga","text":"","title":"Lab 2: EC (GA)"},{"location":"archive/202208/lab2/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"archive/202208/lab2/#genetic-algorithm","text":"Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"archive/202208/lab2/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14","title":"Feature encoding"},{"location":"archive/202208/lab2/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"archive/202208/lab2/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"archive/202208/lab2/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"archive/202208/lab2/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"archive/202208/lab2/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"archive/202208/lab2/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"archive/202208/lab2/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Combining all functions"},{"location":"archive/202208/lab3/","text":"Lab 3: EC (PSO) Particle swarm optimisation Objective develop a Python function to perform global best particle swarm optimisation Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\) Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? Optional How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Lab 3: EC (PSO)"},{"location":"archive/202208/lab3/#lab-3-ec-pso","text":"","title":"Lab 3: EC (PSO)"},{"location":"archive/202208/lab3/#particle-swarm-optimisation","text":"","title":"Particle swarm optimisation"},{"location":"archive/202208/lab3/#objective","text":"develop a Python function to perform global best particle swarm optimisation","title":"Objective"},{"location":"archive/202208/lab3/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/202208/lab3/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\)","title":"Problem to solve"},{"location":"archive/202208/lab3/#particle-swarm-optimisation_1","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/202208/lab3/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/202208/lab3/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/202208/lab3/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/202208/lab3/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/202208/lab3/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/202208/lab3/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/202208/lab3/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/202208/lab3/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/202208/lab3/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/202208/lab3/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : del space_ax . lines [ 1 ] space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/202208/lab3/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/202208/lab3/#exercise","text":"Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect?","title":"Exercise"},{"location":"archive/202208/lab3/#optional","text":"How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Optional"},{"location":"archive/202208/lab4/","text":"Lab 4: EC (ACO) Ant colony optimisation Objective to develop a Python function to perform ant colony optimisation on a problem Problem to solve We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" }); Problem formulation The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ] Initiating ACO algorithm We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )] Identify path of each ant In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = [] Evaporation In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road Deposition In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone Termination conditions We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage Loop until termination Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ... Visualisation Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 ) Evaluate effect of parameters Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Lab 4: EC (ACO)"},{"location":"archive/202208/lab4/#lab-4-ec-aco","text":"","title":"Lab 4: EC (ACO)"},{"location":"archive/202208/lab4/#ant-colony-optimisation","text":"","title":"Ant colony optimisation"},{"location":"archive/202208/lab4/#objective","text":"to develop a Python function to perform ant colony optimisation on a problem","title":"Objective"},{"location":"archive/202208/lab4/#problem-to-solve","text":"We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" });","title":"Problem to solve"},{"location":"archive/202208/lab4/#problem-formulation","text":"The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ]","title":"Problem formulation"},{"location":"archive/202208/lab4/#initiating-aco-algorithm","text":"We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )]","title":"Initiating ACO algorithm"},{"location":"archive/202208/lab4/#identify-path-of-each-ant","text":"In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = []","title":"Identify path of each ant"},{"location":"archive/202208/lab4/#evaporation","text":"In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road","title":"Evaporation"},{"location":"archive/202208/lab4/#deposition","text":"In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone","title":"Deposition"},{"location":"archive/202208/lab4/#termination-conditions","text":"We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage","title":"Termination conditions"},{"location":"archive/202208/lab4/#loop-until-termination","text":"Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ...","title":"Loop until termination"},{"location":"archive/202208/lab4/#visualisation","text":"Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 )","title":"Visualisation"},{"location":"archive/202208/lab4/#evaluate-effect-of-parameters","text":"Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Evaluate effect of parameters"},{"location":"archive/202208/lab5/","text":"Lab 5: ANN (Supervised learning) Artificial neural networks Objective to construct an multi-layer perceptron classifier using the scikit-learn Python library Load data to be learned We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine () Examine the dataset We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ()) Split data into training and testing sets scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth. Data preprocessing From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset? Construct the ANN model As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train ) Predictions The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test ) Evaluation With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report? Parameters of the fitted model The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs Visualisation the neural network Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp ) Compare the weights before and after the training At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations. Effect of parameters Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202208/lab5/#lab-5-ann-supervised-learning","text":"","title":"Lab 5: ANN (Supervised learning)"},{"location":"archive/202208/lab5/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202208/lab5/#objective","text":"to construct an multi-layer perceptron classifier using the scikit-learn Python library","title":"Objective"},{"location":"archive/202208/lab5/#load-data-to-be-learned","text":"We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine ()","title":"Load data to be learned"},{"location":"archive/202208/lab5/#examine-the-dataset","text":"We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ())","title":"Examine the dataset"},{"location":"archive/202208/lab5/#split-data-into-training-and-testing-sets","text":"scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth.","title":"Split data into training and testing sets"},{"location":"archive/202208/lab5/#data-preprocessing","text":"From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset?","title":"Data preprocessing"},{"location":"archive/202208/lab5/#construct-the-ann-model","text":"As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 )","title":"Construct the ANN model"},{"location":"archive/202208/lab5/#train-the-model","text":"The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train )","title":"Train the model"},{"location":"archive/202208/lab5/#predictions","text":"The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test )","title":"Predictions"},{"location":"archive/202208/lab5/#evaluation","text":"With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report?","title":"Evaluation"},{"location":"archive/202208/lab5/#parameters-of-the-fitted-model","text":"The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs","title":"Parameters of the fitted model"},{"location":"archive/202208/lab5/#visualisation-the-neural-network","text":"Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp )","title":"Visualisation the neural network"},{"location":"archive/202208/lab5/#compare-the-weights-before-and-after-the-training","text":"At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations.","title":"Compare the weights before and after the training"},{"location":"archive/202208/lab5/#effect-of-parameters","text":"Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Effect of parameters"},{"location":"archive/202208/lab6/","text":"Lab 6: ANN (Hyperplane) Artificial neural networks Objective to visualise the hyperplanes of a neural network configuration for better understanding Data preparation We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target Setup the first configuration for neural network Data preprocessing Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the ANN model Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train ) Visualise the classification of a fitted model Prepare the figure and axis 1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) Visualisation function Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis Visualise the result Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Setup the second, third, and more neural network configurations We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test . Consider more input features We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the model 1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train ) Visualise decision area with more input features We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features? Additional There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202208/lab6/#lab-6-ann-hyperplane","text":"","title":"Lab 6: ANN (Hyperplane)"},{"location":"archive/202208/lab6/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202208/lab6/#objective","text":"to visualise the hyperplanes of a neural network configuration for better understanding","title":"Objective"},{"location":"archive/202208/lab6/#data-preparation","text":"We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target","title":"Data preparation"},{"location":"archive/202208/lab6/#setup-the-first-configuration-for-neural-network","text":"","title":"Setup the first configuration for neural network"},{"location":"archive/202208/lab6/#data-preprocessing","text":"Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Data preprocessing"},{"location":"archive/202208/lab6/#construct-and-train-the-ann-model","text":"Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train )","title":"Construct and train the ANN model"},{"location":"archive/202208/lab6/#visualise-the-classification-of-a-fitted-model","text":"","title":"Visualise the classification of a fitted model"},{"location":"archive/202208/lab6/#prepare-the-figure-and-axis","text":"1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 )","title":"Prepare the figure and axis"},{"location":"archive/202208/lab6/#visualisation-function","text":"Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis","title":"Visualisation function"},{"location":"archive/202208/lab6/#visualise-the-result","text":"Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test )","title":"Visualise the result"},{"location":"archive/202208/lab6/#setup-the-second-third-and-more-neural-network-configurations","text":"We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test .","title":"Setup the second, third, and more neural network configurations"},{"location":"archive/202208/lab6/#consider-more-input-features","text":"We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Consider more input features"},{"location":"archive/202208/lab6/#construct-and-train-the-model","text":"1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train )","title":"Construct and train the model"},{"location":"archive/202208/lab6/#visualise-decision-area-with-more-input-features","text":"We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features?","title":"Visualise decision area with more input features"},{"location":"archive/202208/lab6/#additional","text":"There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Additional"},{"location":"archive/202208/lab7/","text":"Lab 7: Fuzzy Systems Fuzzy Systems Objective to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202208/lab7/#lab-7-fuzzy-systems","text":"","title":"Lab 7: Fuzzy Systems"},{"location":"archive/202208/lab7/#fuzzy-systems","text":"","title":"Fuzzy Systems"},{"location":"archive/202208/lab7/#objective","text":"to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system","title":"Objective"},{"location":"archive/202208/lab7/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"archive/202208/lab7/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/202208/lab7/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/202208/lab7/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/202208/lab7/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/202208/lab7/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/202208/lab7/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/202208/lab7/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Fuzzy tipping recommendation system"},{"location":"archive/202309/lab1/","text":"Lab 1: Refresh on Python Amoeba community Assuming a new amoeba takes one month to grow, and from the second month onwards, it takes one month to duplicate itself to create a new amoeba. Given that there is one new amoeba at the beginning of the first month, this is the progression of the number of amoeba in different months. Month 0 : 1 (new) Month 1 : 1 (grown) Month 2 : 1 (grown) + 1 (new) = 2 Month 3 : 2 (grown) + 1 (new) = 3 Month 4 : 3 (grown) + 2 (new) = 5 Month 5 : 5 (grown) + 3 (new) = 8 Month 6 : 8 (grown) + 5 (new) = 13 Month 7 : 13 (grown) + 8 (new) = 21 ... Hint Note the pattern of the sequence Write a function that takes the month number as input argument and provides the number of amoeba at the beginning of that month as output. 1 2 3 def numberofamoeba ( month ): ... return number_of_amoeba Write a function to take the same input argument as numberofamoeba but instead of giving the number of amoeba at that month as output, provide the whole sequence of amoeba number starting from the beginning. For example, if month is 4 , the output of the function should be the list of [1,1,2,3,5] 1 2 3 def numberofamoebaseq ( month ): ... return number_seq Create a scatter plot to plot the sequence of amoeba number from month 0 to month 100. Hint import matplotlib.pyplot as plt to use the Python visualisation library Matplotlib. Scatter plot can be produced with plt.scatter(...) . Fibonacci and Golden Ratio The above sequence of number is also known as the Fibonacci sequence. Note A Fibonacci sequence may or may not include a 0 as the first element of the series, i.e. 0,1,1,2,3,5,8,... instead of 1,1,2,3,5,8 . Plot the ratio between every two consecutive numbers in the Fibonacci sequence. For Fibonacci sequence of 1,1,2,3,5,8,13,21 , plot the line of \\(\\frac{1}{1}\\) , \\(\\frac{2}{1}\\) , \\(\\frac{3}{2}\\) , \\(\\frac{5}{3}\\) , \\(\\frac{8}{5}\\) , \\(\\frac{13}{8}\\) , \\(\\frac{21}{13}\\) . Note The longer the Fibonacci sequence you use, the closer is the value of the ratio between two consecutive numbers to be the golden ratio. Generate a series of coordinates following the algorithm: Start from (0,0) . Get the next Fibonacci number, i.e. 1 . Add (+1,+1) to the previous point (0,0) to get (1,1) . Get the next Fibonacci number, i.e. 1 . Add (+1,-1) to the previous point (1,1) to get (2,0) . Get the next Fibonacci number, i.e. 2 . Add (-2,-2) to the previous point (2,0) to get (0,-2) . Get the next Fibonacci number, i.e. 3 . Add (-3,+3) to the previous point (0,-2) to get (-3,1) . Continue with the next Fibonacci number and update the coordinates with the sequence of the signs (+,+), (+,-), (-,-), (-,+) . The process will create a spiral in the following manner. The sequence of the signs produce the change in directions, and the fibonacci number provides the distance. (0,0) (1,1) (2,0) (0,-2) (-3,1) Create a line plot of the series of coordinates. If the lines are smoothen, it would form the golden spiral which can be found in pinecorns, seashells, and hurricanes. Additional If you are interested in how we may plot arc to connect the points instead of using straight lines, you can refer to Additional: plot arc to form golden spiral . Random selection based on probability For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption Additional: plot arc to form golden spiral The golden spiral can be produced by drawing the arc connecting every consecutive coordinates. (0,0) (1,1) (2,0) (0,-2) (-3,1) (1,0) (1,0) To draw the arc using matplotlib library, we need to identify the center of each arc. The arc and its corresponding center are colored with the same color in the previous figure. matplotlib . patches . Arc ( xy , # center of the arc width , # length of horizontal axis, height , # length of vertical axis, angle , # rotation of the ellipse in degrees (counterclockwise) theta1 , # starting angle of the arc in degrees theta2 # end angle of the arc in degrees ) The centers of every arc can be genrated from the sequence of coordinates using the following function: function generatecenters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generatecenters ( coordinates ): centers = [] for i , coord in enumerate ( coordinates ): if i == 0 : # add coordinate to list of center centers . append ([ coord [ 0 ], coord [ 1 ]]) elif i == 1 : # change x-coordinate of the first center centers [ - 1 ][ 0 ] = coord [ 0 ] else : centers . append ([ centers [ - 1 ][ 0 ], centers [ - 1 ][ 1 ]]) if i % 2 == 0 : # use y-coordinate as y for new center centers [ - 1 ][ 1 ] = coord [ 1 ] else : # use x-coordinate as x for new center centers [ - 1 ][ 0 ] = coord [ 0 ] return centers The coordinates is the list of coordinates generated from Fibonacci and Golden Ratio step 3. The following function will then use the generated centers of the arc, and the Fibonacci sequence generated from numberofamoebaseq to draw the arc. The handler of the axis needs to be passed into the function as well. function plotspiral 1 2 3 4 5 6 7 8 9 10 11 12 13 def plotspiral ( axis , series , centers ): angle = 90 for number , center in zip ( series , centers ): arc = Arc ( xy = center , width = 2 * number , height = 2 * number , angle = angle , theta1 = 0 , theta2 = 90 ) axis . add_patch ( arc ) angle -= 90 In your script, you will first generate the Fibonacci sequence, use the sequence to generate coordinates, generate centers of arcs, and plot the arcs to form the spiral. 1 2 3 4 5 6 7 n = 80 number_seq = numberofamoebaseq ( n ) coordinates = generatecoordinatesfromseries ( number_seq ) centers = generatecenters ( coordinates ) plt . figure () plt . scatter ( ... ) # or plt.plot(...) to plot the coordinates as in Fibonacci and Golden Ratio step 4 plotspiral ( plt . gca (), number_seq , centers ) # plt.gca() returns handle of the current axis Limitation Due to the limitation of matplotlib, the spiral plotting only works for the Fibonacci sequence with length less than 93.","title":"Lab 1: Refresh on Python"},{"location":"archive/202309/lab1/#lab-1-refresh-on-python","text":"","title":"Lab 1: Refresh on Python"},{"location":"archive/202309/lab1/#amoeba-community","text":"Assuming a new amoeba takes one month to grow, and from the second month onwards, it takes one month to duplicate itself to create a new amoeba. Given that there is one new amoeba at the beginning of the first month, this is the progression of the number of amoeba in different months. Month 0 : 1 (new) Month 1 : 1 (grown) Month 2 : 1 (grown) + 1 (new) = 2 Month 3 : 2 (grown) + 1 (new) = 3 Month 4 : 3 (grown) + 2 (new) = 5 Month 5 : 5 (grown) + 3 (new) = 8 Month 6 : 8 (grown) + 5 (new) = 13 Month 7 : 13 (grown) + 8 (new) = 21 ... Hint Note the pattern of the sequence Write a function that takes the month number as input argument and provides the number of amoeba at the beginning of that month as output. 1 2 3 def numberofamoeba ( month ): ... return number_of_amoeba Write a function to take the same input argument as numberofamoeba but instead of giving the number of amoeba at that month as output, provide the whole sequence of amoeba number starting from the beginning. For example, if month is 4 , the output of the function should be the list of [1,1,2,3,5] 1 2 3 def numberofamoebaseq ( month ): ... return number_seq Create a scatter plot to plot the sequence of amoeba number from month 0 to month 100. Hint import matplotlib.pyplot as plt to use the Python visualisation library Matplotlib. Scatter plot can be produced with plt.scatter(...) .","title":"Amoeba community"},{"location":"archive/202309/lab1/#fibonacci-and-golden-ratio","text":"The above sequence of number is also known as the Fibonacci sequence. Note A Fibonacci sequence may or may not include a 0 as the first element of the series, i.e. 0,1,1,2,3,5,8,... instead of 1,1,2,3,5,8 . Plot the ratio between every two consecutive numbers in the Fibonacci sequence. For Fibonacci sequence of 1,1,2,3,5,8,13,21 , plot the line of \\(\\frac{1}{1}\\) , \\(\\frac{2}{1}\\) , \\(\\frac{3}{2}\\) , \\(\\frac{5}{3}\\) , \\(\\frac{8}{5}\\) , \\(\\frac{13}{8}\\) , \\(\\frac{21}{13}\\) . Note The longer the Fibonacci sequence you use, the closer is the value of the ratio between two consecutive numbers to be the golden ratio. Generate a series of coordinates following the algorithm: Start from (0,0) . Get the next Fibonacci number, i.e. 1 . Add (+1,+1) to the previous point (0,0) to get (1,1) . Get the next Fibonacci number, i.e. 1 . Add (+1,-1) to the previous point (1,1) to get (2,0) . Get the next Fibonacci number, i.e. 2 . Add (-2,-2) to the previous point (2,0) to get (0,-2) . Get the next Fibonacci number, i.e. 3 . Add (-3,+3) to the previous point (0,-2) to get (-3,1) . Continue with the next Fibonacci number and update the coordinates with the sequence of the signs (+,+), (+,-), (-,-), (-,+) . The process will create a spiral in the following manner. The sequence of the signs produce the change in directions, and the fibonacci number provides the distance. (0,0) (1,1) (2,0) (0,-2) (-3,1) Create a line plot of the series of coordinates. If the lines are smoothen, it would form the golden spiral which can be found in pinecorns, seashells, and hurricanes. Additional If you are interested in how we may plot arc to connect the points instead of using straight lines, you can refer to Additional: plot arc to form golden spiral .","title":"Fibonacci and Golden Ratio"},{"location":"archive/202309/lab1/#random-selection-based-on-probability","text":"For this section. assume the random.random() function selects the random number with even probability. Consider a coin tossing event. If the probabilities of getting a head or a tail are even, i.e. 50%. Create a Python function which will simulate the coin tossing event and return the result as head or tail . 1 2 3 def tossCoin (): ... return headOrTail If the probabilities of getting a head or a tail are not even, with head as 20% and tail as 80%, how would you change the Python function you created previously to adapt to this coin? Consider the event of selecting one option out of three options randomly. The probability of choosing option A is 20%, B is 50%, and C is 30%. Create a Python function to simulate the random selection of the options. 1 2 3 def chooseFromThree (): ... return selectedOption","title":"Random selection based on probability"},{"location":"archive/202309/lab1/#additional-plot-arc-to-form-golden-spiral","text":"The golden spiral can be produced by drawing the arc connecting every consecutive coordinates. (0,0) (1,1) (2,0) (0,-2) (-3,1) (1,0) (1,0) To draw the arc using matplotlib library, we need to identify the center of each arc. The arc and its corresponding center are colored with the same color in the previous figure. matplotlib . patches . Arc ( xy , # center of the arc width , # length of horizontal axis, height , # length of vertical axis, angle , # rotation of the ellipse in degrees (counterclockwise) theta1 , # starting angle of the arc in degrees theta2 # end angle of the arc in degrees ) The centers of every arc can be genrated from the sequence of coordinates using the following function: function generatecenters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generatecenters ( coordinates ): centers = [] for i , coord in enumerate ( coordinates ): if i == 0 : # add coordinate to list of center centers . append ([ coord [ 0 ], coord [ 1 ]]) elif i == 1 : # change x-coordinate of the first center centers [ - 1 ][ 0 ] = coord [ 0 ] else : centers . append ([ centers [ - 1 ][ 0 ], centers [ - 1 ][ 1 ]]) if i % 2 == 0 : # use y-coordinate as y for new center centers [ - 1 ][ 1 ] = coord [ 1 ] else : # use x-coordinate as x for new center centers [ - 1 ][ 0 ] = coord [ 0 ] return centers The coordinates is the list of coordinates generated from Fibonacci and Golden Ratio step 3. The following function will then use the generated centers of the arc, and the Fibonacci sequence generated from numberofamoebaseq to draw the arc. The handler of the axis needs to be passed into the function as well. function plotspiral 1 2 3 4 5 6 7 8 9 10 11 12 13 def plotspiral ( axis , series , centers ): angle = 90 for number , center in zip ( series , centers ): arc = Arc ( xy = center , width = 2 * number , height = 2 * number , angle = angle , theta1 = 0 , theta2 = 90 ) axis . add_patch ( arc ) angle -= 90 In your script, you will first generate the Fibonacci sequence, use the sequence to generate coordinates, generate centers of arcs, and plot the arcs to form the spiral. 1 2 3 4 5 6 7 n = 80 number_seq = numberofamoebaseq ( n ) coordinates = generatecoordinatesfromseries ( number_seq ) centers = generatecenters ( coordinates ) plt . figure () plt . scatter ( ... ) # or plt.plot(...) to plot the coordinates as in Fibonacci and Golden Ratio step 4 plotspiral ( plt . gca (), number_seq , centers ) # plt.gca() returns handle of the current axis Limitation Due to the limitation of matplotlib, the spiral plotting only works for the Fibonacci sequence with length less than 93.","title":"Additional: plot arc to form golden spiral"},{"location":"archive/202309/lab2/","text":"Lab 2: Fuzzy Systems Fuzzy Systems Objective to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system Note Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy Fuzzy control system for a train Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf Initialise inputs and outputs Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules) Define membership functions for fuzzy sets of variables The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors. Define rules The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] . Construct the fuzzy control system The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train ) View the control/output space The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle ) Fuzzy tipping recommendation system A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Lab 2: Fuzzy Systems"},{"location":"archive/202309/lab2/#lab-2-fuzzy-systems","text":"","title":"Lab 2: Fuzzy Systems"},{"location":"archive/202309/lab2/#fuzzy-systems","text":"","title":"Fuzzy Systems"},{"location":"archive/202309/lab2/#objective","text":"to construct a Mamdani fuzzy system using the scikit-fuzzy Python library to evaluate the result of the constructed fuzzy system","title":"Objective"},{"location":"archive/202309/lab2/#note","text":"Install the scikit-fuzzy Python library in your environment before proceeding with the lab. 1 conda install - c conda - forge scikit - fuzzy","title":"Note"},{"location":"archive/202309/lab2/#fuzzy-control-system-for-a-train","text":"Consider a fuzzy control system to control the brake and throttle of a train based on the speed of the train and the distance of the train to the next stop. Import the skfuzzy , skfuzzy.control , and numpy . 1 2 3 import numpy as np from skfuzzy import control as ctrl from skfuzzy import membership as mf","title":"Fuzzy control system for a train"},{"location":"archive/202309/lab2/#initialise-inputs-and-outputs","text":"Speed and distance are the inputs of the system whereas brake and throttle are the outputs. The ranges for the variables are: Variable Range Speed 0 - 85 km/h Distance 0 - 3000 m Brake 0 - 100% Throttle 0 - 100% As the inputs will be the antecedents of the rules, construct the variables speed and distance as skfuzzy.control.Antecedent objects. 1 speed = ctrl . Antecedent ( np . arange ( 0 , 85 , 0.1 ), 'speed' ) The initialisation function for skfuzzy.control.Antecedent object takes 2 arguments, the first is the universe of the variable, i.e. the values the variables can take, the second is the label of the variable. The initialisation function for skfuzzy.control.Consequent is similar. The label and the range of the variable can be accessed using .label and .universe respectively. Task : Initialise the variables distance as Antecedent object, and brake and throttle as Consequent objects. (Outputs of the system will be consequents of the rules)","title":"Initialise inputs and outputs"},{"location":"archive/202309/lab2/#define-membership-functions-for-fuzzy-sets-of-variables","text":"The fit vectors of the fuzzy sets for the linguistic variables are given as follows: speed (0 to 85 km/h) Linguistic value Fit vector Stopped (1/0, 0/2) Very slow (0/1, 1/2.5, 0/4) Slow (0/2.5, 1/6.5, 0/10.5) Medium fast (0/6.5, 1/26.5, 0/46.5) Fast (0/26.5, 1/70, 1/85) distance (0 to 3000 m) Linguistic value Fit vector At (1/0, 0/2) Very near (0/1, 1/3, 0/5) Near (0/3, 1/101.5, 0/200) Medium far (0/100, 1/1550, 0/3000) Far (0/1500, 1/2250, 1/3000) brake (0 to 100%) Linguistic value Fit vector No (1/0, 0/40) Very slight (0/20, 1/50, 0/80) Slight (0/70, 1/83.5, 0/97) Medium (0/95, 1/97, 0/99) Full (0/98, 1/100) throttle (0 to 100%) Linguistic value Fit vector No (1/0, 0/2) Very slight (0/1, 1/3, 0/5) Slight (0/3, 1/16.5, 0/30) Medium (0/20, 1/50, 0/80) Full (0/60, 1/80, 1/100) The skfuzzy.membership module provides the following membership functions: Membership function Description skfuzzy.membership.dsigmf(x, b1, c1, b2, c2) Difference of two fuzzy sigmoid membership functions skfuzzy.membership.gauss2mf(x, mean1, ...) Gaussian fuzzy membership function of two combined Gaussians skfuzzy.membership.gaussmf(x, mean, sigma) Gaussian fuzzy membership function skfuzzy.membership.gbellmf(x, a, b, c) Generalized Bell function fuzzy membership generator skfuzzy.membership.piecemf(x, abc) Piecewise linear membership function (particularly used in FIRE filters) skfuzzy.membership.pimf(x, a, b, c, d) Pi-function fuzzy membership generator skfuzzy.membership.psigmf(x, b1, c1, b2, c2) Product of two sigmoid membership functions skfuzzy.membership.sigmf(x, b, c) The basic sigmoid membership function generator skfuzzy.membership.smf(x, a, b) S-function fuzzy membership generator skfuzzy.membership.trapmf(x, abcd) Trapezoidal membership function generator skfuzzy.membership.trimf(x, abc) Triangular membership function generator skfuzzy.membership.zmf(x, a, b) Z-function fuzzy membership generator The fit vector of a linguitic value can be assigned to a linguistic variable using 1 2 speed [ 'stopped' ] = mf . trimf ( speed . universe , [ 0 , 0 , 2 ]) speed [ 'very slow' ] = mf . trimf ( speed . universe , [ 1 , 2.5 , 4 ]) Task : Assign all fuzzy sets to the linguistic variables. The fuzzy set diagram of a linguistic variable can be viewed using .view() 1 speed . view () Task : Check if the fuzzy set diagrams match the fit vectors.","title":"Define membership functions for fuzzy sets of variables"},{"location":"archive/202309/lab2/#define-rules","text":"The rules for this system are displayed in the following fuzzy association memory (FAM) representaion table. Distance At Very near Near Medium far Far Speed Stopped Full brake No throttle Full brake Very slight throttle Very slow Full brake No throttle Medium brake Very slight throttle Slight brake Very slight throttle Slow Full brake No throttle Medium brake Very slight throttle Very slight brake Slight throttle Medium fast Very slight brake Medium throttle No brake Full throttle Fast Very slight brake Medium throttle No brake Full throttle Rule can be defined using skfuzzy.control.Rule(antecedent, consequent, label) . To define the first rule, i.e. if distance is 'at' and speed is 'stopped', then full brake and no throttle, 1 rule1 = ctrl . Rule ( distance [ 'at' ] & speed [ 'stopped' ], ( brake [ 'full' ], throttle [ 'no' ])) If the antecedent consists of multiple parts, they can be combined using operators | (OR), & (AND), and ~ (NOT). If the consequent consists of multiple parts, they can be combined as a list / tuple . Task : Define all the rules. Then combine all the rules in a list , i.e. rules = [rule1, rule2, ...] .","title":"Define rules"},{"location":"archive/202309/lab2/#construct-the-fuzzy-control-system","text":"The train control system can be constructed with 1 train_ctrl = ctrl . ControlSystem ( rules = rules ) A skfuzzy.control.ControlSystemSimulation object is needed to simulate the control system to obtain the outputs given certain inputs. 1 train = ctrl . ControlSystemSimulation ( control_system = train_ctrl ) To obtain the values for brake and throttle given that speed is 30 km/h and distance is 6 m, 1 2 3 4 5 6 7 8 9 10 11 12 # define the values for the inputs train . input [ 'speed' ] = 30 train . input [ 'distance' ] = 2000 # compute the outputs train . compute () # print the output values print ( train . output ) # to extract one of the outputs print ( train . output [ 'brake' ]) To view the results in the graph, 1 2 brake . view ( sim = train ) throttle . view ( sim = train )","title":"Construct the fuzzy control system"},{"location":"archive/202309/lab2/#view-the-controloutput-space","text":"The control/output space allows us to identify if the outputs fit our expectation. Construct an empty 3D space with 100-by-100 x-y grid. 1 2 3 4 x , y = np . meshgrid ( np . linspace ( speed . universe . min (), speed . universe . max (), 100 ), np . linspace ( distance . universe . min (), distance . universe . max (), 100 )) z_brake = np . zeros_like ( x , dtype = float ) z_throttle = np . zeros_like ( x , dtype = float ) Loop through every point and identify the value of brake and throttle of each point. As the specified rules are not exhaustive, i.e. some input combinations do not activate any rule, we will set the output of such input combinations to be float('inf') . 1 2 3 4 5 6 7 8 9 10 11 for i , r in enumerate ( x ): for j , c in enumerate ( r ): train . input [ 'speed' ] = x [ i , j ] train . input [ 'distance' ] = y [ i , j ] try : train . compute () except : z_brake [ i , j ] = float ( 'inf' ) z_throttle [ i , j ] = float ( 'inf' ) z_brake [ i , j ] = train . output [ 'brake' ] z_throttle [ i , j ] = train . output [ 'throttle' ] Plot the result in a 3D graph using the matplotlib.pyplot library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def plot3d ( x , y , z ): fig = plt . figure () ax = fig . add_subplot ( 111 , projection = '3d' ) ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , cmap = 'viridis' , linewidth = 0.4 , antialiased = True ) ax . contourf ( x , y , z , zdir = 'z' , offset =- 2.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'x' , offset = x . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . contourf ( x , y , z , zdir = 'y' , offset = y . max () * 1.5 , cmap = 'viridis' , alpha = 0.5 ) ax . view_init ( 30 , 200 ) plot3d ( x , y , z_brake ) plot3d ( x , y , z_throttle )","title":"View the control/output space"},{"location":"archive/202309/lab2/#fuzzy-tipping-recommendation-system","text":"A fuzzy expert system is designed to identify the percentage of tips a customer will give based on the service and the food the customer received. The system has service and food as inputs, and tips as output. The fit vectors of the fuzzy sets for the linguistic variables are given as follows: service (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) food (0 to 10) Linguistic value Fit vector Poor (1/0, 0/5) Average (0/0, 1/5, 0/10) Good (0/5, 1/10) tips (0 to 30%) Linguistic value Fit vector Low (1/0, 0/15) Medium (0/0, 1/15, 0/30) High (0/15, 1/30) The rules are displayed in the following fuzzy association memory (FAM) representaion table. Food Poor Average Good Service Poor low tips low tips medium tips Average low tips medium tips high tips Good medium tips high tips high tips Task : Construct the fuzzy inference system. Task : Modify the membership functions of the input 'service' to Linguistic value Fit vector Poor (1/0, 0/3) Average (0/2, 1/5, 0/8) Good (0/6, 1/10)","title":"Fuzzy tipping recommendation system"},{"location":"archive/202309/lab3/","text":"Lab 3: EC (GA) Binary-to-gray code conversion Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes. Genetic algorithm Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination Feature encoding In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14 Population initialisation A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values. Fitness calculation The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero. Selection as parents From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]] Crossover Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code. Mutation Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated. Repeat until termination The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses. Combining all functions The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Lab 3: EC (GA)"},{"location":"archive/202309/lab3/#lab-3-ec-ga","text":"","title":"Lab 3: EC (GA)"},{"location":"archive/202309/lab3/#binary-to-gray-code-conversion","text":"Binary string is often used in the implementation of genetic algorithm. However, the downside of using a binary code is that the Hamming distance between two adjacent values is not consistent. This situation is solved by using a Gray code in place of a binary code. numpy provides the function of binary_repr to convert a decimal value to its corresponding binary code. Create a function to take the input of a binary code and return the correponding Gray code of the binary code. Create a function to calculate the Hamming distance between two binary strings (two binary codes or two Gray codes). Consider a sequence of decimal values of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] . Convert the sequence to a series of binary codes. Identify and plot ( example of a line plot ) the Hamming distances between the adjacent values. Repeat the previous step with Gray codes instead of binary codes.","title":"Binary-to-gray code conversion"},{"location":"archive/202309/lab3/#genetic-algorithm","text":"Consider the following problem: Problem You are given a sheet of paper with width w and height h . Your task is to cut the paper into squares of equal size. The aim of the task is to have as many squares as possible, and to have the area of each square as large as possible. An optimisation problem can always be phrased in the form of to optimise ... such that it maximises/minimises ... In this problem, what is the parameter to be optimised and what are the parameters to be maximised or minimised? Let x denotes the length of the sides of a square. Design a fitness function such that higher fitness corresponds to larger number of squares and large area. If the number of squares (that can be cut out) is zero, or the area of the square is zero, the fitness will be zero. feature encoding population initialisation selection as parents crossover mutation offspring (next generation population) repeat from fitnexx calculation until termination","title":"Genetic algorithm"},{"location":"archive/202309/lab3/#feature-encoding","text":"In this problem as we only have one feature, i.e. the side length of the square, each chromosome consists of the value of the side length of the square. We will encode the chromosome in the form of Gray code. Create two functions value2gray and gray2value to convert a decimal value to its Gray code and vice versa. 1 2 3 4 5 6 7 8 9 def value2gray ( value ): # this function converts a decimal value to its gray code representation ... return gray def gray2value ( gray ): # this function converts a gray code representation to its decimal value ... return value Add the following code snippet to the end of the code to test your functions. 1 2 3 if __name__ == \"__main__\" : print ( value2gray ( 10 )) print ( gray2value ( \"1001\" )) After running the file as a script, you should see the following output. 1 2 1111 14","title":"Feature encoding"},{"location":"archive/202309/lab3/#population-initialisation","text":"A population is randomly generated according to the defined population size. Create a function to generate randomly a population of size pop_size with each value lies between the range of pop_min to pop_max . 1 2 3 4 def generatePopulation ( pop_size , pop_min , pop_max ): # this function generate the first generation randomly based on the population size and the range of the value of each chromosome ... return population This function and all the functions created after this should be placed before the if __name__ == \"__main__\": code block. [Optional testing] You can test the function by changing the __main__ code block to 1 2 if __name__ == \"__main__\" : print ( generatePopulation ( 8 , 0 , 10 )) The printed output should be a series of 8 chromosomes displayed as decimal values.","title":"Population initialisation"},{"location":"archive/202309/lab3/#fitness-calculation","text":"The fitness function was designed at the beginning of this section . Define a function that takes the input of a chromosome (as decimal value) and returns the fitness of the chromosome. 1 2 3 4 def calculateFitness ( value ): # this function calculates the fitness of a chromosome from the decimal value of the chromosome ... return fitness 2. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( calculateFitness ( 5 )) The printed output should be the fitness of a chromosome of value 5, which would be a decimal value larger than zero.","title":"Fitness calculation"},{"location":"archive/202309/lab3/#selection-as-parents","text":"From the list of the chromosomes, we will select the chromosome pairs as parents. As we will be using one-point crossover, each pair of parents will produce exactly two offsprings. Therefore for population size of pop_size , we need pop_size/2 pairs of parents. Define a function that takes the inputs of the current population and the total number of chromosomes in current population, and returns the chromosome pairs which will act as parents. The selection process is performed with the roulette wheel selection. The same chromosome can be selected more than once. 1 2 3 def selectParents ( chromosomes , pop_size ): ... return parent_pairs [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( selectParents ([ 13 , 8 , 14 , 7 ], 6 )) The printed output should be 3 parent pairs, for example, 1 [[13, 8], [8, 14], [13, 7]]","title":"Selection as parents"},{"location":"archive/202309/lab3/#crossover","text":"Define a function that takes a parent pair and returns a pair of offspring after performing one-point crossover. 1 2 3 4 def crossover ( parents ): # this function takes a parent pair and perform one-point crossover to produce a pair of offspring ... return offsprings [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( crossover ([ 13 , 9 ])) The printed output should be a pair of offsprings, for example, 1 [10, 14] 13 is 1011 and 9 is 1101 in Gray code, the offsprings 10 is 1111 and 14 is 1001 in Gray code.","title":"Crossover"},{"location":"archive/202309/lab3/#mutation","text":"Each gene in all chromosomes has the same mutation probability p_mutation . Define a function that takes a chromosome and the mutation probability p_mutation as the inputs, and returns the mutated chromosome. 1 2 3 4 def mutate ( chromosome , p_mutation ): # this function mutates each gene of a chromosome based on the mutation probability ... return mutated 3. [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( mutate ( 15 , 0.1 )) The printed output should be the mutated or unmutated chromosome, for example, 14 . 15 is 1000 and 14 is 1001 in Gray code. In the example output, the last bit is mutated.","title":"Mutation"},{"location":"archive/202309/lab3/#repeat-until-termination","text":"The common termination criteria are the maximum number of iterations and the distance among the fitnesses of the chromosomes of the latest population. Define a function that calculates one metric to measure the distance among the fitnesses of the chromosomes, i.e. how far the fitnesses of all the chromosomes are from each other. 1 2 3 4 def findOverallDistance ( chromosomes ): # this function takes the input of the current population and returns the overall distance among fitnesses of all chromosomes ... return overall_distance [Optional] Test the function with 1 2 if __name__ == \"__main__\" : print ( findOverallDistance ([ 13 , 11 , 14 , 7 ])) The printed output should be a decimal value that represents the overall distance of fitnesses.","title":"Repeat until termination"},{"location":"archive/202309/lab3/#combining-all-functions","text":"The functions we have created can be combined with the following code snippet to execute the genetic algorithm to solve the problem defined at the beginning of this section . Consider the width and the height of the sheet of paper to be 20cm and 15cm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 if __name__ == \"__main__\" : # main function ## parameter definition pop_size = 10 pop_min = 1 #1cm pop_max = 10 #10cm curr_iter = 0 max_iter = 100 min_overalldistance = 0.5 p_mutation = 0.05 ## initialise population population = [] population . append ( generatePopulation ( pop_size , pop_min , pop_max )) while ( curr_iter < max_iter and findOverallDistance ( population [ - 1 ]) > min_overalldistance ): curr_iter += 1 ## select parent pairs parents = selectParents ( population [ - 1 ], len ( population [ - 1 ])) ## perform crossover offsprings = [] for p in parents : new_offsprings = crossover ( p ) for o in new_offsprings : offsprings . append ( o ) ## perform mutation mutated = [ mutate ( offspring , p_mutation ) for offspring in offsprings ] ## update current population population . append ( mutated )","title":"Combining all functions"},{"location":"archive/202309/lab4/","text":"Lab 4: EC (PSO) Particle swarm optimisation Objective develop a Python function to perform global best particle swarm optimisation Setup for Spyder If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel . Problem to solve Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\) Particle swarm optimisation particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination Parameter definition With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10 Create a class for particle As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position Fitness function Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness Initialise particles Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed. Update personal best Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position. Update global best Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle. Update velocity Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity Update particle position As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits Create a loop (until termination) Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ]) Visualisation Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : space_ax . lines [ 1 ] . remove () space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value Evaluation Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' ) Exercise Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect? Optional How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Lab 4: EC (PSO)"},{"location":"archive/202309/lab4/#lab-4-ec-pso","text":"","title":"Lab 4: EC (PSO)"},{"location":"archive/202309/lab4/#particle-swarm-optimisation","text":"","title":"Particle swarm optimisation"},{"location":"archive/202309/lab4/#objective","text":"develop a Python function to perform global best particle swarm optimisation","title":"Objective"},{"location":"archive/202309/lab4/#setup-for-spyder","text":"If you are using Spyder for this lab, go to Tools > Preferences > IPython console > Graphics and set Backend to Automatic . Restart kernel by going to Consoles > Restart kernel .","title":"Setup for Spyder"},{"location":"archive/202309/lab4/#problem-to-solve","text":"Solve the following problem using global best particle swarm optimisation: Problem Find the value of x to minimise the function \\(f(x) = (x+100)(x+50)(x)(x-20)(x-60)(x-100)\\) for \\(-100 < x < 100\\)","title":"Problem to solve"},{"location":"archive/202309/lab4/#particle-swarm-optimisation_1","text":"particles initialisation personal best identification global best identification velocity calculation position update repeat from personal best identification until termination","title":"Particle swarm optimisation"},{"location":"archive/202309/lab4/#parameter-definition","text":"With global best particle swarm optimisaton, the position update function is given by \\[x_i(t+1) = x_i(t) + v_i(t+1)\\] and the velocity update function is \\[v_i(t+1) = v_i(t) + \\alpha_1\\beta_1(t) \\Big( p_i(t) - x_i(t) \\Big) + \\alpha_2\\beta_2(t)\\Big(p_g(t) - x_i(t)\\Big)\\] \u03b1 1 and \u03b1 2 are acceleration constants that are fixed throughout the algorithm. Define a small value for \u03b1 1 and \u03b1 2 , for example 0.1 . 1 alpha = [ 0.1 , 0.1 ] \u03b2 1 (t) and \u03b2 2 (t) are random values between 0 and 1 that are regenerated every iteration. Therefore no definition is required. Also, define the number of particles to run the algorithm with. 1 n_particle = 10 Place the definition of these variables in the __main__ block. 1 2 3 if __name__ == '__main__' : alpha = [ 0.1 , 0.1 ] n_particle = 10","title":"Parameter definition"},{"location":"archive/202309/lab4/#create-a-class-for-particle","text":"As each particle is an individual, create a Particle class to hold the data of the particle's current position, velocity, and personal best position. 1 2 3 4 5 class Particle : def __init__ ( self , position = 0 , velocity = 0 ): self . position = position self . velocity = velocity self . best_position = position","title":"Create a class for particle"},{"location":"archive/202309/lab4/#fitness-function","text":"Fitness function is how we can compare different particles. As our goal is to minimise f(x) as stated in the beginning , we will use f(x) as our fitness function. By using f(x) in minimisation problem, it implies that the lower the value of f(x), the better the particle it is. The value of x is the position of the particle. Define the fitness function as a Python function. 1 2 3 def fit_fcn ( position ): ... return fitness","title":"Fitness function"},{"location":"archive/202309/lab4/#initialise-particles","text":"Particles are initialised with random positions within the constraints. At initialisation, we may assume that the initial velocities of all the particles. It is possible to initialise particles with non-zero velocities. For now, we will stick to zero initial velocities. Define a Python function that takes the input of the number of particles and the limits of the positions to initialise and return a list of objects of class Particle . Each particle has random position within the limits and zero velocity. 1 2 3 4 def initialise_particles ( n_ptc , position_limits ): # position_limits is a list of two values. The first value is the lower boundary and the second value is the upper boundary. ... return particles Remember to test your function before proceed.","title":"Initialise particles"},{"location":"archive/202309/lab4/#update-personal-best","text":"Create a method in the class Particle to update the best_position if necessary. 1 2 3 4 5 6 7 8 9 class Particle : def __init__ ( ... ): ... def update_personal_best ( self ): # 1. calculate the fitnesses of the best_position and the particle's current position # 2. compare the fitnesses and determine if the current position is better than the best_position # 3. update if necessary # 4. no return statement is required If the new position has a lower fitness, i.e. the new position is better than the best position, update the best_position to hold the value of the new position.","title":"Update personal best"},{"location":"archive/202309/lab4/#update-global-best","text":"Initiate a variable named global_best_position with the value None in the __main__ block. Create a function that takes two positions as inputs, compare them, and return the better position of the two. 1 2 3 4 def compareFitness ( pos1 , pos2 ): # 1. calculate the fitness of pos1 and pos2 # 2. compare to determine the better position return betterpos We will later use this function to compare the current global best position with the personal best position of each particle.","title":"Update global best"},{"location":"archive/202309/lab4/#update-velocity","text":"Create a method in the class Particle to update the velocity given \u03b1 1 , \u03b1 2 , \u03b2 1 , \u03b2 2 , and the global best position. 1 2 3 4 5 6 7 8 9 10 11 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( self , alpha , beta , glob_best_pos ): # alpha is a list of two values. we will access alpha_1 and alpha_2 by alpha[0] and alpha[1] respectively. This also applies to beta. # the current position, current velocity, and personal best position of the particle can be accessed by self.position, self.velocity, and self.best_position # assign the particle's velocity with the updated velocity","title":"Update velocity"},{"location":"archive/202309/lab4/#update-particle-position","text":"As updating a particle position only require information from within the particle object and the limits of the position, create a method called update_position in the class Particle taking the input of the limits of the position. 1 2 3 4 5 6 7 8 9 10 11 12 13 class Particle : def __init__ ( ... ): ... def update_personal_best ( ... ): ... def update_velocity ( ... ): ... def update_position ( self , position_limits ): self . position = self . position + self . velocity # how should you solve the problem of the position (x) going out of the limits","title":"Update particle position"},{"location":"archive/202309/lab4/#create-a-loop-until-termination","text":"Consider the following termination criteria: exceeding 200 iterations fitnesses of all particles are close positions of all particles are close Create a function to calculate the average difference between the mean fitness and the fitness of each particle. 1 2 3 4 5 def calc_avg_fit_diff ( particles ): # 1. calculate mean fitness of all particles # 2. calculate the difference between the mean fitness and the fitness of each particle # 3. calculate the average of the differences obtained from step 2 return avg_fit_diff Create a function to calculate the average difference between the mean position and the position of each particle. 1 2 3 4 5 def calc_avg_pos_diff ( particles ): # 1. calculate mean position of all particles # 2. calculate the difference between the mean position and the position of each particle # 3. calculate the average of the differences obtained from step 2 return avg_pos_diff Create a loop (in the __main__ block) to execute the global best particle swarm optimisation (gbest PSO) until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 if __name__ == '__main__' : # parameter initialisation alpha = [ 0.1 , 0.1 ] n_particle = 10 global_best_position = None position_limits = [ - 100 , 100 ] # termination threshold iteration = 0 max_iter = 200 min_avg_fit_diff = 0.1 min_avg_pos_diff = 0.1 # initialise particles particles = initialise_particles ( n_particle , position_limits ) while ( ... ): # how should you define the termination criteria here? print ( iteration , [ round ( x . position , 2 ) for x in particles ]) for particle in particles : # update personal best particle . update_personal_best () # update global best if global_best_position == None : global_best_position = particle . position else : global_best_position = compareFitness ( global_best_position , particle . position ) # generate beta randomly for current iteration beta = [ random . random (), random . random ()] for particle in particles : # update velocity particle . update_velocity ( alpha , beta , global_best_position ) # update position particle . update_position ( position_limits ) iteration += 1 # display results print ( iteration , [ round ( x . position , 2 ) for x in particles ])","title":"Create a loop (until termination)"},{"location":"archive/202309/lab4/#visualisation","text":"Let's add a few lines to visualise particles \"flying\" towards to optimal position. import the visualisation library 1 import matplotlib.pyplot as plt add the following lines just before the while loop in the last code block in the previous section . 1 2 3 4 5 space_ax = plt . axes () space_ax . plot ( list ( range ( * position_limits )),[ fit_fcn ( x ) for x in range ( * position_limits )]) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) space_ax . set_xlabel ( \"Position\" ) space_ax . set_ylabel ( \"Fitness\" ) add the following lines between line 14 and line 15 in the last code block in the previous section , as well as after line 33. 1 2 3 4 5 if len ( space_ax . lines ) > 1 : space_ax . lines [ 1 ] . remove () space_ax . plot ([ x . position for x in particles ], [ fit_fcn ( x . position ) for x in particles ], 'go' ) space_ax . set_title ( \"Position of particles in iteration {} \" . format ( iteration )) plt . pause ( 0.5 ) # pause the program for 0.5 second; if graph changes too quickly, increase this value; you can also speed up the process by decreasing this value","title":"Visualisation"},{"location":"archive/202309/lab4/#evaluation","text":"Store the values of the variables at each iteration for analysis and evaluation. position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . position_list = [ position ] def update_position ( ... ): ... self . position_list . append ( self . position ) velocity of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . velocity_list = [ velocity ] def update_velocity ( ... ): ... self . velocity_list . append ( self . velocity ) personal best position of each particle at each iteration (add the new line of code to the end of the methods) 1 2 3 4 5 6 7 8 class Particle : def __init__ ( ... ): ... self . best_position_list = [] def update_personal_best ( ... ): ... self . best_position_list . append ( self . best_position ) global best position at each iteration 1 2 3 4 5 6 7 8 9 if __init__ == '__main__' : # parameter initialisation ... global_best_position_list = [] ... global_best_position = ... global_best_position_list . append ( global_best_position ) # take note on the indentation # generate beta randomly for current iteration ... Visualise the progression of these variables by adding the following code to the end of the __main__ block. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ pos_fig , position_axes ] = plt . subplots ( 4 , 1 , sharex = True ) position_axes [ 0 ] . set_title ( \"Position of each particle\" ) position_axes [ 1 ] . set_title ( \"Fitness of each particle\" ) position_axes [ 2 ] . set_title ( \"Boxplot of position at each iteration\" ) position_axes [ 3 ] . set_title ( \"Boxplot of fitness at each iteration\" ) position_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ vel_fig , velocity_axes ] = plt . subplots ( 2 , 1 , sharex = True ) velocity_axes [ 0 ] . set_title ( \"Velocity of each particle\" ) velocity_axes [ 1 ] . set_title ( \"Boxplot for velocity at each iteration\" ) velocity_axes [ 1 ] . set_xlabel ( \"Iteration\" ) [ p_best_fig , personal_best_axes ] = plt . subplots ( 4 , 1 , sharex = True ) personal_best_axes [ 0 ] . set_title ( \"Personal best position of each particle\" ) personal_best_axes [ 1 ] . set_title ( \"Personal best fitness of each particle\" ) personal_best_axes [ 2 ] . set_title ( \"Boxplot of personal best position at each iteration\" ) personal_best_axes [ 3 ] . set_title ( \"Boxplot of personal best fitness at each iteration\" ) personal_best_axes [ 3 ] . set_xlabel ( \"Iteration\" ) [ g_best_fig , global_best_axes ] = plt . subplots ( 2 , 1 , sharex = True ) global_best_axes [ 0 ] . set_title ( \"Global best position\" ) global_best_axes [ 1 ] . set_title ( \"Fitness for global best position\" ) global_best_axes [ 1 ] . set_xlabel ( \"Iteration\" ) for particle in particles : iteration_list = list ( range ( len ( particle . position_list ))) position_axes [ 0 ] . plot ( iteration_list , particle . position_list , '-o' ) position_axes [ 1 ] . plot ( iteration_list , [ fit_fcn ( x ) for x in particle . position_list ], '-o' ) velocity_axes [ 0 ] . plot ( iteration_list , particle . velocity_list , '-o' ) personal_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], particle . best_position_list , '-o' ) personal_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in particle . best_position_list ], '-o' ) position_axes [ 2 ] . boxplot ([[ p . position_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) position_axes [ 3 ] . boxplot ([[ fit_fcn ( p . position_list [ i ]) for p in particles ] for i in iteration_list ], positions = iteration_list ) velocity_axes [ 1 ] . boxplot ([[ p . velocity_list [ i ] for p in particles ] for i in iteration_list ], positions = iteration_list ) personal_best_axes [ 2 ] . boxplot ([[ p . best_position_list [ i ] for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) personal_best_axes [ 3 ] . boxplot ([[ fit_fcn ( p . best_position_list [ i ]) for p in particles ] for i in iteration_list [: - 1 ]], positions = iteration_list [: - 1 ]) global_best_axes [ 0 ] . plot ( iteration_list [: - 1 ], global_best_position_list , '-o' ) global_best_axes [ 1 ] . plot ( iteration_list [: - 1 ], [ fit_fcn ( x ) for x in global_best_position_list ], '-o' )","title":"Evaluation"},{"location":"archive/202309/lab4/#exercise","text":"Multiply the velocity memory, \\(v_i(t)\\) , with a value between 0 and 1, let's say 0.5. How does the process change? This is the effect of inertia weight. Reduce the value of \\(\\alpha_1\\) to 0.05 while maintaining \\(\\alpha_2\\) at 0.1 and investigate the effect. Reduce the value of \\(\\alpha_1\\) to 0. How does this affect the result? Modify such that \\(\\alpha_1\\) is larger than \\(\\alpha_2\\) . What's the effect?","title":"Exercise"},{"location":"archive/202309/lab4/#optional","text":"How may you modify the formulae for particles with two variables, in which the fitness function is defined as \\(f(x,y) = x^2 + y^2\\) ?","title":"Optional"},{"location":"archive/202309/lab5/","text":"Lab 5: EC (ACO) Ant colony optimisation Objective to develop a Python function to perform ant colony optimisation on a problem Problem to solve We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" }); Problem formulation The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ] Initiating ACO algorithm We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )] Identify path of each ant In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = [] Evaporation In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road Deposition In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone Termination conditions We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage Loop until termination Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ... Visualisation Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 ) Evaluate effect of parameters Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Lab 5: EC (ACO)"},{"location":"archive/202309/lab5/#lab-5-ec-aco","text":"","title":"Lab 5: EC (ACO)"},{"location":"archive/202309/lab5/#ant-colony-optimisation","text":"","title":"Ant colony optimisation"},{"location":"archive/202309/lab5/#objective","text":"to develop a Python function to perform ant colony optimisation on a problem","title":"Objective"},{"location":"archive/202309/lab5/#problem-to-solve","text":"We will use ant colony optimisation to solve the Nick's route-finding problem in Romania. The problem is a route finding problem to identify the best (cheapest) route to travel from Arad to Bucharest. The road map of Romania is provided as follows: 75 71 151 140 118 111 70 75 120 146 80 99 97 138 101 211 90 85 98 86 142 92 87 Arad Zerind Oradea Sibiu Fagaras Rimnicu Vilcea Pitesti Craiova Drobeta Mehadia Lugoj Timisoara Bucharest Giurgiu Urziceni Hirsova Eforie Vaslui Iasi Neamt new Vue({ el: \"#romania\" });","title":"Problem to solve"},{"location":"archive/202309/lab5/#problem-formulation","text":"The coordinates of each town are provided as follows. This will be used later for the purpose of visualisation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 location_list = [ # [x,y,name] [ 75 , 125 , 'Arad' ], [ 100 , 75 , 'Zerind' ], [ 125 , 25 , 'Oradea' ], [ 265 , 175 , 'Sibiu' ], [ 425 , 175 , 'Fagaras' ], [ 320 , 230 , 'Rimnicu Vilcea' ], [ 475 , 310 , 'Pitesti' ], [ 350 , 465 , 'Craiova' ], [ 185 , 450 , 'Drobeta' ], [ 190 , 390 , 'Mehadia' ], [ 185 , 335 , 'Lugoj' ], [ 85 , 280 , 'Timisoara' ], [ 640 , 390 , 'Bucharest' ], [ 575 , 485 , 'Giurgiu' ], [ 745 , 340 , 'Urziceni' ], [ 875 , 340 , 'Hirsova' ], [ 935 , 440 , 'Eforie' ], [ 850 , 225 , 'Vaslui' ], [ 760 , 120 , 'Iasi' ], [ 625 , 60 , 'Neamt' ] ] Then define the travel cost between connected cities. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 step_cost = [ [ 'Arad' , 'Zerind' , 75 ], [ 'Zerind' , 'Oradea' , 71 ], [ 'Oradea' , 'Sibiu' , 151 ], [ 'Sibiu' , 'Arad' , 140 ], [ 'Sibiu' , 'Fagaras' , 99 ], [ 'Sibiu' , 'Rimnicu Vilcea' , 80 ], [ 'Fagaras' , 'Bucharest' , 211 ], [ 'Bucharest' , 'Giurgiu' , 90 ], [ 'Bucharest' , 'Pitesti' , 101 ], [ 'Pitesti' , 'Rimnicu Vilcea' , 97 ], [ 'Rimnicu Vilcea' , 'Craiova' , 146 ], [ 'Craiova' , 'Pitesti' , 138 ], [ 'Craiova' , 'Drobeta' , 120 ], [ 'Drobeta' , 'Mehadia' , 75 ], [ 'Mehadia' , 'Lugoj' , 70 ], [ 'Lugoj' , 'Timisoara' , 111 ], [ 'Arad' , 'Timisoara' , 118 ], [ 'Bucharest' , 'Urziceni' , 85 ], [ 'Urziceni' , 'Vaslui' , 142 ], [ 'Vaslui' , 'Iasi' , 92 ], [ 'Iasi' , 'Neamt' , 87 ], [ 'Urziceni' , 'Hirsova' , 98 ], [ 'Hirsova' , 'Eforie' , 86 ] ] We will define two class, City and Road . An object of class City has the attributes of name (the name of the city), roads (an array of references to the roads connected to the current city), and coordinates (coordinates of the cities). 1 2 3 4 5 6 7 8 9 10 11 12 class City : def __init__ ( self , name ): self . name = name self . roads = [] self . coordinates = [] def set_coordinates ( self , coordinates ): self . coordinates = coordinates def add_road ( self , road ): if road not in self . roads : self . roads . append ( road ) An object of class Road has the attributes of connected_cities (an array of references to the cities connected through this road), cost (the step cost of this road), and pheromone (the pheromone on this road). 1 2 3 4 5 class Road : def __init__ ( self , connected_cities , cost , pheromone = 0 ): self . connected_cities = connected_cities self . cost = cost self . pheromone = pheromone We will construct the list of City objects and Road objects from the information provided by the question, i.e. information in location_list and step_cost . The following code block should be in the main code block. 1 2 3 4 5 6 7 8 9 10 cities = {} for coord1 , coord2 , name in location_list : cities [ name ] = City ( name ) cities [ name ] . set_coordinates ([ coord1 , coord2 ]) roads = [] for city1 , city2 , cost in step_cost : road = Road ([ cities [ city1 ], cities [ city2 ]], cost ) cities [ city1 ] . add_road ( road ) cities [ city2 ] . add_road ( road ) roads . append ( road ) In the main code block, define the origin and destination cities. 1 2 origin = cities [ 'Arad' ] destination = cities [ 'Bucharest' ]","title":"Problem formulation"},{"location":"archive/202309/lab5/#initiating-aco-algorithm","text":"We then define the parameters for ACO, i.e. number of ants, n_ant , pheromone influence constant, alpha , and evaporation rate, rho . 1 2 3 n_ant = 10 alpha = 1 rho = 0.1 Add the method set_pheromone to the class Road . 1 2 3 4 5 6 class Road : def __init__ ( ... ): ... def set_pheromone ( self , pheromone ): self . pheromone = pheromone Set the initial pheromone of each road to 0.01. 1 2 3 4 # in main block initial_pheromone = 0.01 for road in roads : road . set_pheromone ( initial_pheromone ) Define the class Ant . 1 2 3 4 class Ant : def __init__ ( self ): self . cities = [] # cities the ant passes through, in sequence self . path = [] # roads the ant uses, in sequence Initiate n_ants ants. 1 ants = [ Ant () for _ in range ( n_ant )]","title":"Initiating ACO algorithm"},{"location":"archive/202309/lab5/#identify-path-of-each-ant","text":"In the Ant class, define a method to identify the path by taking the inputs of the available roads, the origin, the destination, and the pheromone influence constant \u03b1. 1 2 3 4 5 6 7 8 class Ant : def __init__ ( ... ): ... def get_path ( self , origin , destination , alpha ): # 1. append origin to the self.cities # 2. if the last city is not destination, search for the next city to go # 3. after getting to the destination, remove the loop within the path, i.e. if there are repeated cities in self.cities, remove the cities and the roads in between the repetition Define a method to calculate the path length. 1 2 3 4 5 6 7 8 9 10 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( self ): # calculate path length based on self.path return path_length As the path of each ant will be reset every iteration, define a method that reset the path and cities . 1 2 3 4 5 6 7 8 9 10 11 12 13 class Ant : def __init__ ( ... ): ... def get_path ( ... ): ... def get_path_length ( ... ): ... def reset ( self ): self . path = [] self . cities = []","title":"Identify path of each ant"},{"location":"archive/202309/lab5/#evaporation","text":"In the Road class, define a method to evaporate the pheromone by taking the input of evaporation rate \u03c1. 1 2 3 4 5 6 7 8 9 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( self , rho ): # update the pheromone of the road","title":"Evaporation"},{"location":"archive/202309/lab5/#deposition","text":"In the Road class, define a method to calculate the updated pheromone after pheromone deposition by taking the input of all the ants. We will use the following pheromone deposition formula for ant \\(k\\) on road \\(i\\) : \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k}\\] 1 2 3 4 5 6 7 8 9 10 11 12 13 class Road : def __init__ ( ... ): ... def set_pheromone ( ... ): ... def evaporate_pheromone ( ... ): ... def deposit_pheromone ( self , ants ): # 1. search for ants that uses the raod # 2. deposit pheromone using the inversely proportionate relationship between path length and deposited pheromone","title":"Deposition"},{"location":"archive/202309/lab5/#termination-conditions","text":"We will use the following conditions as the termination conditions: maximum iteration of 200 if \u226590% of the ants use the same path Define a function to calculate the percentage of the most dominant path. 1 2 3 def get_percentage_of_dominant_path ( ants ): ... return percentage","title":"Termination conditions"},{"location":"archive/202309/lab5/#loop-until-termination","text":"Create a loop to iterate until termination. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # termination threshold max_iteration = 200 percentage_of_dominant_path = 0.9 iteration = 0 while ... : # termination conditions # loop through all the ants to identify the path of each ant for ant in ants : # reset the path of the ant ant . reset () # identify the path of the ant ant . get_path ( origin , destination , alpha ) # loop through all roads for road in roads : # evaporate the pheromone on the road road . evaporate_pheromone ( rho ) # deposit the pheromone road . deposit_pheromone ( ants ) # increase iteration count iteration += 1 # after exiting the loop, return the most occurred path as the solution ...","title":"Loop until termination"},{"location":"archive/202309/lab5/#visualisation","text":"Define the following functions: 1 2 3 4 5 6 7 8 9 10 import matplotlib.pyplot as plt ... def create_graph ( cities ): fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) cities_x = [ city . coordinates [ 0 ] for key , city in cities . items ()] cities_y = [ city . coordinates [ 1 ] for key , city in cities . items ()] ax . scatter ( cities_x , cities_y ) ax . set_aspect ( aspect = 1.0 ) return ax 1 2 3 4 5 6 7 8 9 def draw_pheromone ( ax , roads ): lines = [] for road in roads : from_coord = road . connected_cities [ 0 ] . coordinates to_coord = road . connected_cities [ 1 ] . coordinates coord_x = [ from_coord [ 0 ], to_coord [ 0 ]] coord_y = [ from_coord [ 1 ], to_coord [ 1 ]] lines . append ( ax . plot ( coord_x , coord_y , c = 'k' , linewidth = road . pheromone ** 2 )) return lines Add the following lines to the main code block just before the while loop (loop until termination). 1 2 ax = create_graph ( cities ) lines = draw_pheromone ( ax , roads ) Add the following lines to after iteration += 1 . 1 2 3 4 5 # visualise for l in lines : del l lines = draw_pheromone ( ax , roads ) plt . pause ( 0.05 )","title":"Visualisation"},{"location":"archive/202309/lab5/#evaluate-effect-of-parameters","text":"Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{1}{L_k^{1.5}}\\] What is the effect of this? Modify the pheromone depositing formula to \\[\\Delta\\tau_{i,k} = \\frac{5}{L_k}\\] What is the effect of this? Investigate the effect of number of ants n_ant . Investigate the effect of pheromone influence constant \\(\\alpha\\) alpha . Investigate the effect of evaporation rate \\(\\rho\\) rho .","title":"Evaluate effect of parameters"},{"location":"archive/202309/lab6/","text":"Lab 6: ANN (Supervised learning) Artificial neural networks Objective to construct an multi-layer perceptron classifier using the scikit-learn Python library Load data to be learned We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine () Examine the dataset We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ()) Split data into training and testing sets scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth. Data preprocessing From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset? Construct the ANN model As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train ) Predictions The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test ) Evaluation With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report? Parameters of the fitted model The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs Visualisation the neural network Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp ) Compare the weights before and after the training At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations. Effect of parameters Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Lab 6: ANN (Supervised learning)"},{"location":"archive/202309/lab6/#lab-6-ann-supervised-learning","text":"","title":"Lab 6: ANN (Supervised learning)"},{"location":"archive/202309/lab6/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202309/lab6/#objective","text":"to construct an multi-layer perceptron classifier using the scikit-learn Python library","title":"Objective"},{"location":"archive/202309/lab6/#load-data-to-be-learned","text":"We will be constructing an artificial neural network and use it to perform prediction. The data we will be using today is the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset is available from the scikit-learn library. Therefore we will first import the module. 1 from sklearn import datasets To load the data, 1 data = datasets . load_wine ()","title":"Load data to be learned"},{"location":"archive/202309/lab6/#examine-the-dataset","text":"We will use pandas to get more insight into the dataset. Import pandas and construct a data frame from the input and target values. 1 2 3 import pandas as pd wine = pd . DataFrame ( data . data , columns = data . feature_names ) wine [ 'target' ] = data . target The describe method of a data frame provides the statistical summary of the dataset. 1 print ( wine . describe () . transpose ())","title":"Examine the dataset"},{"location":"archive/202309/lab6/#split-data-into-training-and-testing-sets","text":"scikit-learn library provides a function to split the data into training and testing sets easily. 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , train_size = 0.8 ) We will split the data into 80% training data and 20% testing data. The first argument will be split into the first two outputs, the second argument the second pair, and so on and so forth.","title":"Split data into training and testing sets"},{"location":"archive/202309/lab6/#data-preprocessing","text":"From Examine the dataset , notice that the range of different features are different. This will cause the training algorithm to be difficult to converge. Now we will use the method of standardisation to normalise the data. scikit-learn provides a built-in class to perform the standardisation, StandardScaler . 1 2 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () We will scale the data based on the training data and then apply the scaler to both the training and testing data. 1 2 3 scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Examine the training input data with the following code: 1 print ( pd . DataFrame ( X_train , columns = data . feature_names ) . describe () . transpose ()) Compare this result with the one in Examine the dataset . How are these two datasets different from each other? What did the StandardScaler do to the dataset?","title":"Data preprocessing"},{"location":"archive/202309/lab6/#construct-the-ann-model","text":"As the target values are classes, i.e. 0 , 1 , or 2 , we need a classifier to perform the classification. 1 from sklearn.neural_network import MLPClassifier We will construct a feedforward neural network with 1 hidden layer of 2 neuron and maximum iteration of 1000 . 1 mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 )","title":"Construct the ANN model"},{"location":"archive/202309/lab6/#train-the-model","text":"The model can be trained using the fit method of the classifier. 1 mlp . fit ( X_train , y_train )","title":"Train the model"},{"location":"archive/202309/lab6/#predictions","text":"The model is now trained. We can use the fitted (trained) model to predict the output of the testing data. 1 predictions = mlp . predict ( X_test )","title":"Predictions"},{"location":"archive/202309/lab6/#evaluation","text":"With the prediction results, we can evaluate the performance of the fitted model. scikit-learn library provides some built-in metrics such as confusion matrix and classification report. 1 from sklearn.metrics import confusion_matrix , classification_report confusion matrix 1 print ( confusion_matrix ( y_test , predictions )) classification report 1 print ( classification_report ( y_test , predictions )) What information is provided by the confusion matrix and the classification report?","title":"Evaluation"},{"location":"archive/202309/lab6/#parameters-of-the-fitted-model","text":"The parameters of the fitted model ( mlp ) can be access through its public attributes: attributes definition .coefs_ weight matrix .intercepts_ bias (threshold) vector .n_iter_ number of iterations the solver has ran .n_layers_ number of layers .n_outputs_ number of outputs","title":"Parameters of the fitted model"},{"location":"archive/202309/lab6/#visualisation-the-neural-network","text":"Copy the following code for the function visualise to the beginning of the script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import matplotlib.pyplot as plt def visualise ( mlp ): # get number of neurons in each layer n_neurons = [ len ( layer ) for layer in mlp . coefs_ ] n_neurons . append ( mlp . n_outputs_ ) # calculate the coordinates of each neuron on the graph y_range = [ 0 , max ( n_neurons )] x_range = [ 0 , len ( n_neurons )] loc_neurons = [[[ l , ( n + 1 ) * ( y_range [ 1 ] / ( layer + 1 ))] for n in range ( layer )] for l , layer in enumerate ( n_neurons )] x_neurons = [ x for layer in loc_neurons for x , y in layer ] y_neurons = [ y for layer in loc_neurons for x , y in layer ] # identify the range of weights weight_range = [ min ([ layer . min () for layer in mlp . coefs_ ]), max ([ layer . max () for layer in mlp . coefs_ ])] # prepare the figure fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) # draw the neurons ax . scatter ( x_neurons , y_neurons , s = 100 , zorder = 5 ) # draw the connections with line width corresponds to the weight of the connection for l , layer in enumerate ( mlp . coefs_ ): for i , neuron in enumerate ( layer ): for j , w in enumerate ( neuron ): ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'white' , linewidth = (( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) * 1.2 ) ax . plot ([ loc_neurons [ l ][ i ][ 0 ], loc_neurons [ l + 1 ][ j ][ 0 ]], [ loc_neurons [ l ][ i ][ 1 ], loc_neurons [ l + 1 ][ j ][ 1 ]], 'grey' , linewidth = ( w - weight_range [ 0 ]) / ( weight_range [ 1 ] - weight_range [ 0 ]) * 5 + 0.2 ) To use this function to visualise the neural network, use the following line after the predictions. 1 visualise ( mlp )","title":"Visualisation the neural network"},{"location":"archive/202309/lab6/#compare-the-weights-before-and-after-the-training","text":"At initiation, the weights are not assigned. Therefore we need to train the model once before we can visualise the neural network. We can train the model once using the .partial_fit method. Put the following lines right after the initiation of mlp . 1 2 mlp . partial_fit ( X_train , y_train , np . unique ( data . target )) visualise ( mlp ) Compare the weights of the two visualisations.","title":"Compare the weights before and after the training"},{"location":"archive/202309/lab6/#effect-of-parameters","text":"Investigate the effect of the following parameters on the performance of the neural network and the number of iterations to achieve convergence. number of hidden layers number of neurons in hidden layers splitting ratio of training and testing sets","title":"Effect of parameters"},{"location":"archive/202309/lab7/","text":"Lab 7: ANN (Hyperplane) Artificial neural networks Objective to visualise the hyperplanes of a neural network configuration for better understanding Data preparation We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target Setup the first configuration for neural network Data preprocessing Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the ANN model Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train ) Visualise the classification of a fitted model Prepare the figure and axis 1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 ) Visualisation function Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis Visualise the result Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Setup the second, third, and more neural network configurations We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test . Consider more input features We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Construct and train the model 1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train ) Visualise decision area with more input features We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features? Additional There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Lab 7: ANN (Hyperplane)"},{"location":"archive/202309/lab7/#lab-7-ann-hyperplane","text":"","title":"Lab 7: ANN (Hyperplane)"},{"location":"archive/202309/lab7/#artificial-neural-networks","text":"","title":"Artificial neural networks"},{"location":"archive/202309/lab7/#objective","text":"to visualise the hyperplanes of a neural network configuration for better understanding","title":"Objective"},{"location":"archive/202309/lab7/#data-preparation","text":"We will use the iris data for the training in this lab. 1 2 from sklearn import datasets iris = datasets . load_iris () We will start with using just two input features. 1 2 3 X = [[ d [ 1 ], d [ 2 ]] for d in iris . data ] names = [ iris . target_names [ 1 ], iris . target_names [ 2 ]] Y = iris . target","title":"Data preparation"},{"location":"archive/202309/lab7/#setup-the-first-configuration-for-neural-network","text":"","title":"Setup the first configuration for neural network"},{"location":"archive/202309/lab7/#data-preprocessing","text":"Training and testing sets split 1 2 from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , Y , train_size = 0.8 ) Scale the input data based on the training input data 1 2 3 4 5 from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Data preprocessing"},{"location":"archive/202309/lab7/#construct-and-train-the-ann-model","text":"Construct the model 1 2 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier ( hidden_layer_sizes = ( 2 ), max_iter = 1000 ) Train the model 1 mlp . fit ( X_train , y_train )","title":"Construct and train the ANN model"},{"location":"archive/202309/lab7/#visualise-the-classification-of-a-fitted-model","text":"","title":"Visualise the classification of a fitted model"},{"location":"archive/202309/lab7/#prepare-the-figure-and-axis","text":"1 2 3 import matplotlib.pyplot as plt fig = plt . figure () ax = fig . add_subplot ( 1 , 1 , 1 )","title":"Prepare the figure and axis"},{"location":"archive/202309/lab7/#visualisation-function","text":"Download the vis.py and save it to the same folder as your script. Import all functions under the namespace of vis 1 import vis","title":"Visualisation function"},{"location":"archive/202309/lab7/#visualise-the-result","text":"Use the following code to visualise the decision area of the model. 1 vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test )","title":"Visualise the result"},{"location":"archive/202309/lab7/#setup-the-second-third-and-more-neural-network-configurations","text":"We will investigate the effect of using different activation functions on the hyperplane. The activation functions that are available for MLPClassifier are identity , logistic , tanh , and relu (default). They are explained in the documentation . We can use a for -loop to construct and train the neural network model with different configurations. The same datasets from previous sections are used. 1 2 3 4 5 6 7 8 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( 1 , len ( activation_functions ), i + 1 ) ax . set_title ( actfcn ) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) Apart from the activation functions, let's compare the results of having different number of hidden layers. 1 2 3 4 5 6 7 8 9 10 11 12 activation_functions = [ 'identity' , 'logistic' , 'tanh' , 'relu' ] hidden_layers = [( 3 ), ( 3 , 3 ), ( 3 , 3 , 3 )] fig = plt . figure () for i , actfcn in enumerate ( activation_functions ): for j , hlyr in enumerate ( hidden_layers ): mlp = MLPClassifier ( hidden_layer_sizes = hlyr , activation = actfcn , max_iter = 1000 ) mlp . fit ( X_train , y_train ) ax = fig . add_subplot ( len ( hidden_layers ), len ( activation_functions ), j * len ( activation_functions ) + i + 1 ) ax . set_title ( ' {} , {} , {} ' . format ( actfcn , str ( hlyr ), round ( mlp . score ( X_test , y_test ), 2 ))) vis . vis2d ( ax , mlp , X_train , y_train , X_test , y_test ) ax . set_xticks ([]) ax . set_yticks ([]) mlp.score(X_test, y_test) gives the prediction accuracy of the model on X_test compared against y_test .","title":"Setup the second, third, and more neural network configurations"},{"location":"archive/202309/lab7/#consider-more-input-features","text":"We will now use all the input features instead of two. To prepare the data, 1 2 3 4 X_train , X_test , y_train , y_test = train_test_split ( iris . data , iris . target , train_size = 0.8 ) scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Consider more input features"},{"location":"archive/202309/lab7/#construct-and-train-the-model","text":"1 2 mlp = MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 10000 ) mlp . fit ( X_train , y_train )","title":"Construct and train the model"},{"location":"archive/202309/lab7/#visualise-decision-area-with-more-input-features","text":"We will be using parallel coordinates to display data with more than 2 input features. The following is not a complete parallel coordinates plot but a partial one. 1 2 3 4 5 6 7 fig = plt . figure () axes = vis . vis3d ( fig , mlp , X_train , y_train , X_test , y_test ) for i , a in enumerate ( axes ): a . set_title ( iris . target_names [ i ]) a . set_xticklabels ([]) a . get_yaxis () . set_visible ( False ) axes [ - 1 ] . set_xticklabels ( iris . feature_names ) Is there other alternative to display data with more than 2 input features?","title":"Visualise decision area with more input features"},{"location":"archive/202309/lab7/#additional","text":"There is a tensorflow playground which tries to visualise the training process of a neural network. It's similar to what we did in this lab.","title":"Additional"}]}